{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qTIHQLGqMbzo",
        "d-lsBPOsakWM"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reload Data"
      ],
      "metadata": {
        "id": "qTIHQLGqMbzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### defining functions"
      ],
      "metadata": {
        "id": "jAOiyGhInq2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgRuef6LV4vX",
        "outputId": "7026ff2a-7d0b-400e-d66f-bba0f9e864e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/per-residue-dataset/')"
      ],
      "metadata": {
        "id": "yYWpzJvuVjAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re\n",
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk-fPZIoV_ou",
        "outputId": "049fcb1a-99e6-4c49-b907-7eff0de87e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_energy_scores(scores):\n",
        "    # Assuming 'scores' is a list of energy score values\n",
        "    return [1 if score <= -1 else 0 for score in scores]"
      ],
      "metadata": {
        "id": "uAwYbJiBNtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pickle"
      ],
      "metadata": {
        "id": "xNIkI2LXN154"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the protT5_tokens dictionary\n",
        "with open('protT5_tokens.pkl', 'rb') as file:\n",
        "    protT5_tokens = pickle.load(file)"
      ],
      "metadata": {
        "id": "IpAVMrBxY5WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(protT5_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPLZZ0T7UK-P",
        "outputId": "9a7c2f9f-f82e-4fce-bfdf-a29bb865ee69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9665"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(protT5_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMvVZSZ5Y7Lt",
        "outputId": "01b074b0-6aee-44f9-85ec-6d3f5091982e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9665"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
        "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model = model.half() if device.type == 'cuda' else model.full()\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2brgQ9ITtVxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c515949-9cf6-4fb9-a813-8aa098b93ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe, protT5_embeddings, protT5_tokens):\n",
        "        self.dataframe = dataframe\n",
        "        self.protT5_embeddings = protT5_embeddings\n",
        "        self.protT5_tokens = protT5_tokens\n",
        "\n",
        "        # Determine the maximum lengths\n",
        "        self.max_length_embeddings = max(max(len(self.protT5_embeddings[seq1]), len(self.protT5_embeddings[seq2]))\n",
        "                                         for seq1, seq2 in zip(dataframe['peptide_derived_sequence'], dataframe['protein_derived_sequence']))\n",
        "        self.max_length_tokenized = max(len(self.protT5_tokens[seq]) for seq in dataframe['peptide_derived_sequence'])\n",
        "        self.max_length_scores = max(len(re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', scores)) for scores in dataframe['energy_scores'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        peptide_seq = self.dataframe.iloc[idx]['peptide_derived_sequence']\n",
        "        protein_seq = self.dataframe.iloc[idx]['protein_derived_sequence']\n",
        "        energy_scores = self.dataframe.iloc[idx]['energy_scores']\n",
        "\n",
        "        # Process the energy_scores\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        # Pad the energy scores -- max length of all should be equal\n",
        "        energy_scores_padded = pad(torch.tensor(energy_scores), (0, self.max_length_tokenized - len(energy_scores)), \"constant\", 0)\n",
        "\n",
        "        peptide_embedding = self.protT5_embeddings[peptide_seq]\n",
        "        protein_embedding = self.protT5_embeddings[protein_seq]\n",
        "        tokenized_peptide_seq = self.protT5_tokens[peptide_seq]\n",
        "\n",
        "        # print('max_length_embeddings:', self.max_length_embeddings)\n",
        "        # print('max_length_tokenized:', self.max_length_tokenized)\n",
        "        # print('max_length_scores:', self.max_length_scores)\n",
        "\n",
        "        # Pad the sequences\n",
        "        peptide_embedding_padded = pad(peptide_embedding, (0, 0, 0, self.max_length_embeddings - len(peptide_embedding)), \"constant\", 0)\n",
        "        protein_embedding_padded = pad(protein_embedding, (0, 0, 0, self.max_length_embeddings - len(protein_embedding)), \"constant\", 0)\n",
        "        tokenized_peptide_seq_padded = pad(torch.tensor(tokenized_peptide_seq, dtype=torch.float), (0, self.max_length_tokenized - len(tokenized_peptide_seq)), \"constant\", 0)\n",
        "\n",
        "\n",
        "        # print(peptide_embedding_padded.shape,protein_embedding_padded.shape,energy_scores_padded.shape,tokenized_peptide_seq_padded.shape)\n",
        "        return peptide_embedding_padded, protein_embedding_padded, energy_scores_padded, tokenized_peptide_seq_padded\n",
        "\n",
        "# # Usage\n",
        "# protein_interaction_dataset = ProteinInteractionDataset(dataframe, protT5_embeddings, protT5_tokens)\n",
        "# protein_interaction_dataloader = DataLoader(protein_interaction_dataset, batch_size=your_batch_size)\n"
      ],
      "metadata": {
        "id": "0o_Oh31WYL5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('protT5_embeddings.pkl', 'rb') as file:\n",
        "    protT5_embeddings = pickle.load(file)"
      ],
      "metadata": {
        "id": "4-zJmkj8akNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(protT5_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuOlqLumUPmf",
        "outputId": "0ac77cf9-ff11-47a3-9349-a74e574fc07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9665"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing SnP PPI data"
      ],
      "metadata": {
        "id": "d-lsBPOsakWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/per-residue-dataset/')"
      ],
      "metadata": {
        "id": "6ZU_o2feakWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebfcb0bf-7c25-4cc4-da37-4dff4651aa32",
        "id": "IcIKO1k-akWN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "protT5_embeddings.pkl  testing_dataset.csv   validation_dataset.csv\n",
            "protT5_tokens.pkl      training_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3wp_ZnN-akWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_snp = pd.read_csv('testing_dataset.csv')\n",
        "train_snp = pd.read_csv('training_dataset.csv')\n",
        "val_snp = pd.read_csv('validation_dataset.csv')"
      ],
      "metadata": {
        "id": "-8iGKGCNakWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_snp_data(file_path):\n",
        "    # Read the dataset\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Function to transform energy scores\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            # Replace sequences of spaces/newlines with a comma\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            # Remove a comma after an opening square bracket\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            # Remove leading commas/whitespace\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    # Apply transformations\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    # Calculate lengths for other columns\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    # Calculate matching lengths count (optional, depending on your needs)\n",
        "    snp_df['matching_lengths_count'] = (snp_df['energy_scores_lengths'] == snp_df['peptide_derived_seq_length']).sum()\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "# Applying the preprocessing pipeline to each dataset\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n"
      ],
      "metadata": {
        "id": "jAPH4kHmakWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_snp['protein_derived_sequence'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbba71b-6471-4e9b-9ba6-b9cb985a4ac5",
        "id": "SOB7DZb8akWP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'VWLANPERYGQMQYRYCGKSGLRLPALSLGLWHNFGHVNALESQRAILRKAFDLGITHFDLANNYGPPPGSAEENFGRLLREDFAAYRDELIISTKAGYDMWPGPYGSGGSRKYLLASLDQSLKRMGLEYVDIFYSHRVDENTPMEETASALAHAVQSGKALYVGISSYSPERTQKMVELLREWKIPLLIHQPSYNLLNRWVDKSGLLDTLQNNGVGCIAFTPLAQGLLTGKYLTEANLNSLRLLNEMAQQRGQSMAQMALSWLLKDDRVTSVLIGASRAEQLEENVQALNNLTFSTKELAQIDQHIADGELN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create the *datasets*"
      ],
      "metadata": {
        "id": "u8bPwSMianK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(protT5_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fFxxJ1WT5Xu",
        "outputId": "834cae72-3be9-44ce-8443-09bc6d5b5f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9665"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets with tokenizer\n",
        "train_dataset = ProteinInteractionDataset(train_snp, protT5_embeddings,protT5_tokens)\n",
        "test_dataset = ProteinInteractionDataset(test_snp, protT5_embeddings,protT5_tokens)\n",
        "val_dataset = ProteinInteractionDataset(val_snp, protT5_embeddings,protT5_tokens)\n"
      ],
      "metadata": {
        "id": "usQVlf2NaaUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/flamingo-26-data/')"
      ],
      "metadata": {
        "id": "BxRoInCobAYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_batch_size = 2\n",
        "test_batch_size = 2\n",
        "val_batch_size = 2\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size)\n"
      ],
      "metadata": {
        "id": "WGS0LWw9OCSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader),len(test_dataloader),len(val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTPNHzDMcRXB",
        "outputId": "6cfbea1a-db6e-43b2-f152-2a4ff306de26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motif-guided ProtFlamingo"
      ],
      "metadata": {
        "id": "lTgNrBvYVs8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions + Gated Cross Attn + Perceiver Resampler"
      ],
      "metadata": {
        "id": "QsWWUVuFs8MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# from transformers import RobertaModel  # Assuming use of Hugging Face's transformer models\n",
        "\n",
        "# Helper Functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def set_module_requires_grad_(module, requires_grad):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "def freeze_model_and_make_eval_(model):\n",
        "    model.eval()\n",
        "    set_module_requires_grad_(model, False)\n",
        "\n",
        "# LayerNorm class\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps)\n",
        "\n",
        "# Residual class\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "# SwiGLU activation function\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.silu(x[..., :x.shape[-1] // 2]) * x[..., x.shape[-1] // 2:]\n",
        "\n",
        "# Transformer Block class\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(dim, heads)\n",
        "        self.ln2 = LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            SwiGLU(),\n",
        "            nn.Linear(mlp_dim // 2, dim)\n",
        "        )\n",
        "        self.residual = Residual(self.ln1)\n",
        "        self.feedforward = Residual(self.ln2)\n",
        "        self.expand_dim = nn.Linear(dim, 2 * dim)  # Project to a higher dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() < 3: ### do the 1,1,1024 transformation\n",
        "            # Apply the expansion transformation if x has less than 3 dimensions\n",
        "            x_expanded = self.expand_dim(x)  # Now [2, 2*desired_dim]\n",
        "            x_expanded = x_expanded.view(1, 1, 1024)  # Reshape to [1, 1, 1024]\n",
        "            # x_expanded = nn.LayerNorm(x)\n",
        "            print('x transformed shape in gated cross attn:', x_expanded.shape)\n",
        "            x = self.residual(self.attn(x_expanded, x_expanded, x_expanded)[0])\n",
        "        else:\n",
        "            x = self.residual(self.attn(x, x, x)[0])\n",
        "        print(\"Shape after attention and residual:\", x.shape)  # Debug print\n",
        "        x = self.feedforward(self.mlp(x))\n",
        "        print(\"Shape after feedforward:\", x.shape)  # Debug print\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tDQcNuG8n5nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ypy2EZWLTogB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109ddce4-ae0a-4dc0-a7d7-06f862968fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops-exts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDcDefLVW5-h",
        "outputId": "434c1a52-4810-4cc3-b256-9e56c3045ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops-exts) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096'\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops_exts import rearrange_many, repeat_many\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def FeedForward(dim, mult = 4):\n",
        "    inner_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, inner_dim, bias = False),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(inner_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # print('x shape perciever attn:', x.shape)\n",
        "        # print('latents shape perceiver attn', latents.shape)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        # print('q shape:',q.shape)\n",
        "\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        q = q * self.scale\n",
        "\n",
        "\n",
        "        kv_input = torch.cat((x, latents), dim=1)\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "\n",
        "        # print('k shape:',k.shape)\n",
        "        # print('v shape:',v.shape)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        # print('rearrangement in perceiver cross attn complete...')\n",
        "        # print('q shape:',q.shape)\n",
        "        # print('k shape:',k.shape)\n",
        "        # print('v shape:',v.shape)\n",
        "\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, *, dim, depth, dim_head=64, heads=8, num_latents=64, concatenated_dim=2048):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, concatenated_dim=concatenated_dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=x.shape[0])\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class MaskedCrossAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim=2048, dim_head=64, heads=8, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "        self.only_attend_immediate_media = only_attend_immediate_media\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        b, t, _ = x.shape\n",
        "        _, m, _ = media.shape\n",
        "        h = self.heads\n",
        "\n",
        "        x = self.norm(x)\n",
        "        q = self.to_q(x)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        # No need to reshape media as it's already 3D\n",
        "        k, v = self.to_kv(media).chunk(2, dim=-1)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        q = q * self.scale\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "\n",
        "        if media_locations is not None:\n",
        "            mask = media_locations.unsqueeze(1).unsqueeze(2)\n",
        "            mask = rearrange(mask, 'b n -> b 1 n 1')\n",
        "            sim = sim.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=self.heads)\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, *, dim, dim_head=64, heads=8, ff_mult=4, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.attn = MaskedCrossAttention(dim=dim, concatenated_dim=2048, dim_head=dim_head, heads=heads, only_attend_immediate_media=only_attend_immediate_media)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(x, media, media_locations=media_locations) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "RwoWz-10p__C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ProtFlamingo"
      ],
      "metadata": {
        "id": "Up2gOmc-tC09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(x)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return F.silu(gate) * x\n",
        "\n",
        "class ParallelTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "        attn_inner_dim = dim_head * heads\n",
        "        ff_inner_dim = dim * ff_mult\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, 2* ff_mult * dim),\n",
        "            SwiGLU(),\n",
        "            nn.Linear(ff_mult * dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input to ParallelTransformerBlock:\", x.shape)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        # print(\"After LayerNorm:\", x.shape)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # Rearrange for nn.MultiheadAttention\n",
        "        # print(\"After permute for MultiheadAttention:\", x.shape)\n",
        "\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        # print(\"After MultiheadAttention:\", attn_output.shape)\n",
        "\n",
        "        x = attn_output + x\n",
        "        # print(\"After adding attn_output:\", x.shape)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # Rearrange back\n",
        "        # print(\"After permute back:\", x.shape)\n",
        "\n",
        "        # ff_output = self.ff(x)\n",
        "        # print(\"After FeedForward:\", ff_output.shape)\n",
        "        ff_output = x\n",
        "        for layer in self.ff:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                # print(\"Input to Linear Layer:\", ff_output.shape)\n",
        "                ff_output = layer(ff_output)\n",
        "                # print(\"Output from Linear Layer:\", ff_output.shape)\n",
        "            else:\n",
        "                # Assuming SwiGLU or other non-linear layers don't change shape\n",
        "                ff_output = layer(ff_output)\n",
        "\n",
        "        output = ff_output + x\n",
        "        print(\"Output from ParallelTransformerBlock:\", output.shape)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "2DvByxwAW-rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "HqVTAkETnF01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "XwIGXZqUnTZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\").config.d_model"
      ],
      "metadata": {
        "id": "i00XlIu_l5F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load ProtT5 model\n",
        "# protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "# protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")"
      ],
      "metadata": {
        "id": "v7I4inNjtqVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "class ProtFlamingo(nn.Module):\n",
        "    def __init__(self, num_tokens, depth, dim_head=64, heads=8, ff_mult=4, cross_attn_every=3, perceiver_num_latents=64, perceiver_depth=2, motif_mode=False):\n",
        "        super().__init__()\n",
        "        self.motif_embedding_projection = nn.Embedding(2, 1024) # Assuming binary one-hot encoding, projecting to 1024 dimensions\n",
        "\n",
        "        self.dim = 1024  # Assuming the embedding dimension\n",
        "        self.to_logits = nn.Linear(self.dim, num_tokens)\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "\n",
        "        self.perceiver_resampler = PerceiverResampler(dim=self.dim, depth=perceiver_depth, dim_head=dim_head, heads=heads, num_latents=perceiver_num_latents)\n",
        "        self.expand_seq_len = nn.Linear(dim_head, 983)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            # Note that we're no longer passing 'mlp_dim'\n",
        "            self.layers.append(ParallelTransformerBlock(dim=self.dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult))\n",
        "            if i % cross_attn_every == 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.dim, dim_head=dim_head, heads=heads))\n",
        "\n",
        "\n",
        "    def forward(self, protein_embeddings, motif_encodings):\n",
        "\n",
        "        # Interleave protein and motif embeddings\n",
        "        combined_embeddings,motif_embeddings = self.interleave_embeddings(protein_embeddings, motif_encodings)\n",
        "        print('combined embeddings...')\n",
        "\n",
        "        # Process combined embeddings through the perceiver resampler\n",
        "        processed_protein_embeddings = self.perceiver_resampler(combined_embeddings)\n",
        "        print('processed embeddings...')\n",
        "        print('perceiver resampler output embeddings shape:', processed_protein_embeddings.shape)\n",
        "\n",
        "        for index, layer in enumerate(self.layers):\n",
        "            print('index:', index)\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                # Pass motif_encodings as media\n",
        "                print('output perceiver resampler shape:',processed_protein_embeddings.shape)\n",
        "                print(\"input projected motif embedding shape:\",motif_embeddings.shape)\n",
        "                target_sequence = layer(binder_embeddings, processed_protein_embeddings) # cross attn between binder seq (text) and motif+target (image)\n",
        "                print(f'layer {index} done w gated attn...')\n",
        "                #print('target sequence:',target_sequence)\n",
        "                #print('target sequence shape:',target_sequence.shape)\n",
        "            else:\n",
        "                target_sequence = layer()\n",
        "                print(f'layer {index} done w/out gated attn...')\n",
        "                #print('target sequence:',target_sequence)\n",
        "                #print('target sequence shape:',target_sequence.shape)\n",
        "\n",
        "\n",
        "        # Reshape to merge batch and embedding dimensions\n",
        "        batch_size, seq_length, dim = target_sequence.shape\n",
        "        target_sequence_reshaped = target_sequence.view(batch_size * dim, seq_length)\n",
        "        # Apply linear transformation to expand sequence length\n",
        "        expanded_sequence = self.expand_seq_len(target_sequence_reshaped)\n",
        "        # Reshape back to separate batch and embedding dimensions\n",
        "        expanded_sequence = expanded_sequence.view(batch_size, dim, 983).transpose(1, 2)\n",
        "        print('after last linear transform layer shape:',expanded_sequence.shape)\n",
        "\n",
        "        # Get the logits from the decoder output\n",
        "        logits = self.protT5_model.lm_head(expanded_sequence)\n",
        "        print('logits before argmax:',logits.shape)\n",
        "        predicted_token_ids = logits.argmax(-1)  # Convert logits to token IDs, resulting in shape [2, 64]\n",
        "        print('lm head decoding done...')\n",
        "        print('predidcted token ids:',predicted_token_ids)\n",
        "        print('predicted token ids shape:',predicted_token_ids.shape)\n",
        "\n",
        "        return predicted_token_ids\n",
        "\n",
        "    def interleave_embeddings(self, protein_embeddings, motif_one_hot):\n",
        "        # Map one-hot encoding to embedding space\n",
        "        motif_embeddings = self.motif_embedding_projection(motif_one_hot.long()) # Ensure it's long type for indexing\n",
        "        print('motif_embedding shape projection:',motif_embeddings.shape)\n",
        "        # Interleave embeddings\n",
        "        combined_embeddings = torch.zeros(protein_embeddings.size(0), protein_embeddings.size(1) * 2, protein_embeddings.size(2), device=protein_embeddings.device)\n",
        "        combined_embeddings[:, ::2] = protein_embeddings\n",
        "        combined_embeddings[:, 1::2] = motif_embeddings\n",
        "        print('combined_embeddings shape interleaved:',combined_embeddings.shape)\n",
        "        return combined_embeddings,motif_embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "lLM3qSP9q4mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "rHcPbrabtj5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs = zip(*batch)\n",
        "\n",
        "    seq1_embeddings = pad_sequence(seq1_embeddings, batch_first=True)\n",
        "    seq2_embeddings = pad_sequence(seq2_embeddings, batch_first=True)\n",
        "    one_hot_scores = pad_sequence(one_hot_scores, batch_first=True)\n",
        "    tokenized_seqs = pad_sequence(tokenized_seqs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs\n"
      ],
      "metadata": {
        "id": "tJsZbI-lUGqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import kl_div, log_softmax\n"
      ],
      "metadata": {
        "id": "VU_keTs7E9LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming 'model', 'train_dataloader', 'val_dataloader', 'test_dataloader', and 'criterion' are already defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model = model.half() if device.type == 'cuda' else model.full()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n"
      ],
      "metadata": {
        "id": "7wav5_kRU764"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader.dataset"
      ],
      "metadata": {
        "id": "mViEhzU-pfcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c21836-3a39-41ff-c7ce-884ad36b260a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.ProteinInteractionDataset at 0x79df26776c80>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model, optimizer, and other training components\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Example parameters\n",
        "num_tokens = 28 # protT5 vocab size\n",
        "depth = 3  # Adjust based on model complexity and computational resources"
      ],
      "metadata": {
        "id": "xYk_a6w5r7c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ProtFlamingo(\n",
        "    num_tokens=num_tokens,\n",
        "    depth=depth,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    ff_mult=4,\n",
        "    cross_attn_every=2,\n",
        "    perceiver_num_latents=64,\n",
        "    perceiver_depth=2\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "upD3b5shsN3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def train_epoch_ce(model, data_loader, optimizer, device):\n",
        "    model.train()  # Ensure the model is in training mode\n",
        "    total_loss = 0\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs in data_loader:\n",
        "        seq1_embeddings = seq1_embeddings.to(device).float()\n",
        "        seq2_embeddings = seq2_embeddings.to(device).float()\n",
        "        one_hot_scores = one_hot_scores.to(device).float()\n",
        "        tokenized_seqs = tokenized_seqs.to(device).long()  # Convert tokenized_seqs to long\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        model_output = model(seq1_embeddings, one_hot_scores).to(device)  # Ensure model_output is on the correct device\n",
        "        print('Model output shape:', model_output.shape)\n",
        "        print('Tokenized seqs (target) shape:', tokenized_seqs.shape)\n",
        "\n",
        "        # Calculate loss using CrossEntropyLoss\n",
        "        loss = loss_function(model_output.view(-1, model_output.size(-1)), tokenized_seqs.view(-1))\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print('Current loss:', loss.item())\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Train for one epoch\n",
        "train_loss = train_epoch_ce(model, train_dataloader, optimizer, device)\n",
        "print(f\"Training Epoch: Loss = {train_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "id": "-aFKGjNXGSvo",
        "outputId": "8e7ae244-9433-4bd3-b632-e774262588ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "motif_embedding shape projection: torch.Size([2, 983, 1024])\n",
            "combined_embeddings shape interleaved: torch.Size([2, 1966, 1024])\n",
            "combined embeddings...\n",
            "processed embeddings...\n",
            "perceiver resampler output embeddings shape: torch.Size([2, 64, 1024])\n",
            "index: 0\n",
            "Input to ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "Output from ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "layer 0 done w/out gated attn...\n",
            "index: 1\n",
            "output perceiver resampler shape: torch.Size([2, 64, 1024])\n",
            "input projected motif embedding shape: torch.Size([2, 983, 1024])\n",
            "layer 1 done w gated attn...\n",
            "index: 2\n",
            "Input to ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "Output from ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "layer 2 done w/out gated attn...\n",
            "index: 3\n",
            "Input to ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "Output from ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "layer 3 done w/out gated attn...\n",
            "index: 4\n",
            "output perceiver resampler shape: torch.Size([2, 64, 1024])\n",
            "input projected motif embedding shape: torch.Size([2, 983, 1024])\n",
            "layer 4 done w gated attn...\n",
            "after last linear transform layer shape: torch.Size([2, 983, 1024])\n",
            "logits before argmax: torch.Size([2, 983, 128])\n",
            "lm head decoding done...\n",
            "predidcted token ids: tensor([[46,  1,  7,  ..., 37,  1, 20],\n",
            "        [46,  1,  7,  ..., 37,  1, 20]], device='cuda:0')\n",
            "predicted token ids shape: torch.Size([2, 983])\n",
            "Model output shape: torch.Size([2, 983])\n",
            "Tokenized seqs (target) shape: torch.Size([2, 983])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c28c3e466595>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_ce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Epoch: Loss = {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-c28c3e466595>\u001b[0m in \u001b[0;36mtrain_epoch_ce\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Calculate loss using CrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"host_softmax\" not implemented for 'Long'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def train_epoch_ce(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs in data_loader:\n",
        "        seq1_embeddings = seq1_embeddings.float().to(device)\n",
        "        seq2_embeddings = seq2_embeddings.float().to(device)\n",
        "        one_hot_scores = one_hot_scores.float().to(device)\n",
        "        tokenized_seqs = tokenized_seqs.float().to(device)  # Ensure tokenized_seqs are LongTensors\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        model_output = model(seq1_embeddings, one_hot_scores).float().to(device)\n",
        "        print('tokenized seqs:', tokenized_seqs)\n",
        "        print('tokenized seqs (target) shape:', tokenized_seqs.shape)\n",
        "        print('model output shape:', model_output.shape)\n",
        "\n",
        "        #CrossEntropyLoss\n",
        "        loss = loss_function(model_output, tokenized_seqs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print('Current loss:', loss.item())\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Train for one epoch\n",
        "train_loss = train_epoch_ce(model, train_dataloader, optimizer, device)\n",
        "print(f\"Training Epoch: Loss = {train_loss}\")\n"
      ],
      "metadata": {
        "id": "70J2emtxr6ZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "outputId": "fb36ea49-e0dc-474c-abb2-f505d0dc3f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "motif_embedding shape projection: torch.Size([2, 983, 1024])\n",
            "combined_embeddings shape interleaved: torch.Size([2, 1966, 1024])\n",
            "combined embeddings...\n",
            "processed embeddings...\n",
            "perceiver resampler output embeddings shape: torch.Size([2, 64, 1024])\n",
            "index: 0\n",
            "Input to ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "Output from ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "layer 0 done w/out gated attn...\n",
            "index: 1\n",
            "output perceiver resampler shape: torch.Size([2, 64, 1024])\n",
            "input projected motif embedding shape: torch.Size([2, 983, 1024])\n",
            "layer 1 done w gated attn...\n",
            "index: 2\n",
            "Input to ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "Output from ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "layer 2 done w/out gated attn...\n",
            "index: 3\n",
            "Input to ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "Output from ParallelTransformerBlock: torch.Size([2, 64, 1024])\n",
            "layer 3 done w/out gated attn...\n",
            "index: 4\n",
            "output perceiver resampler shape: torch.Size([2, 64, 1024])\n",
            "input projected motif embedding shape: torch.Size([2, 983, 1024])\n",
            "layer 4 done w gated attn...\n",
            "after last linear transform layer shape: torch.Size([2, 983, 1024])\n",
            "logits before argmax: torch.Size([2, 983, 128])\n",
            "lm head decoding done...\n",
            "predidcted token ids: tensor([[46,  1,  7,  ..., 37,  1, 20],\n",
            "        [46,  1,  7,  ..., 37,  1, 20]], device='cuda:0')\n",
            "predicted token ids shape: torch.Size([2, 983])\n",
            "tokenized seqs: tensor([[14., 12., 20.,  ...,  0.,  0.,  0.],\n",
            "        [19., 17.,  6.,  ...,  0.,  0.,  0.]], device='cuda:0')\n",
            "tokenized seqs (target) shape: torch.Size([2, 983])\n",
            "model output shape: torch.Size([2, 983])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d16c96e9d9d2>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_ce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Epoch: Loss = {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-d16c96e9d9d2>\u001b[0m in \u001b[0;36mtrain_epoch_ce\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#CrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for one epoch\n",
        "train_loss = train_epoch_ce(model, train_dataloader, optimizer, device)\n",
        "print(f\"Training Epoch: Loss = {train_loss}\")"
      ],
      "metadata": {
        "id": "qjmOcsod38bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protT5_tokenizer"
      ],
      "metadata": {
        "id": "so-4BEHlgsV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
