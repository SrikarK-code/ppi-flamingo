{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "collapsed_sections": [
        "mwLOoIWGGeO1",
        "qTIHQLGqMbzo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c058d1f4c2f040ce99f79c4d098cd04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d6c46aa1d24995b83b5a6b1014c5e5",
              "IPY_MODEL_42afd35a144145cfa265376c201b7d21",
              "IPY_MODEL_d9528ec078704c0ab316305d0ce3df0c"
            ],
            "layout": "IPY_MODEL_f940f97a24d642a987130af57718fefa"
          }
        },
        "06d6c46aa1d24995b83b5a6b1014c5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0616e32d6c248098a272c6d381420c0",
            "placeholder": "​",
            "style": "IPY_MODEL_1aaa5709eb8744eba86621ce16224a67",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "42afd35a144145cfa265376c201b7d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9047c95d53142829cca25b9b8ef5855",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05e1fa61b5ac4b1a904c5830d2bca967",
            "value": 25
          }
        },
        "d9528ec078704c0ab316305d0ce3df0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2593fb85d12049a1a443dde518df23eb",
            "placeholder": "​",
            "style": "IPY_MODEL_62c62a59206444a19b2d04311a035c02",
            "value": " 25.0/25.0 [00:00&lt;00:00, 1.66kB/s]"
          }
        },
        "f940f97a24d642a987130af57718fefa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0616e32d6c248098a272c6d381420c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aaa5709eb8744eba86621ce16224a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9047c95d53142829cca25b9b8ef5855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05e1fa61b5ac4b1a904c5830d2bca967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2593fb85d12049a1a443dde518df23eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c62a59206444a19b2d04311a035c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28fd24f1ccee4c218b53c1494425f475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_293c8524a7d64bb3aeaa6a97d0e91fbd",
              "IPY_MODEL_20b561c2a68d4826ba7fddd15b419a67",
              "IPY_MODEL_c4cde7c3c9924ad2b862b813bc00451f"
            ],
            "layout": "IPY_MODEL_5d9fb69f0a704196bdc34f8ca10b7e66"
          }
        },
        "293c8524a7d64bb3aeaa6a97d0e91fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a17d7017efdf41d4bd7c6ae20451e9a7",
            "placeholder": "​",
            "style": "IPY_MODEL_2b750a20848049a3a21d7fc9d733bf4b",
            "value": "spiece.model: 100%"
          }
        },
        "20b561c2a68d4826ba7fddd15b419a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af3d35e159c439e8cec2875852e0cc7",
            "max": 237990,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a7a347daf3493b82d4e754c870d7f7",
            "value": 237990
          }
        },
        "c4cde7c3c9924ad2b862b813bc00451f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752ad9dcfb7b4b4aaeea8bbaf191d184",
            "placeholder": "​",
            "style": "IPY_MODEL_44d16fd5c58f4cfbaa93891a5c266ccc",
            "value": " 238k/238k [00:00&lt;00:00, 1.44MB/s]"
          }
        },
        "5d9fb69f0a704196bdc34f8ca10b7e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17d7017efdf41d4bd7c6ae20451e9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b750a20848049a3a21d7fc9d733bf4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af3d35e159c439e8cec2875852e0cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a7a347daf3493b82d4e754c870d7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "752ad9dcfb7b4b4aaeea8bbaf191d184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d16fd5c58f4cfbaa93891a5c266ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "256a8e461e294dc88c50dc37cbf9ca16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3edbba7acc045d2bdab0246612f64e6",
              "IPY_MODEL_4d377113fa5946bb8545abcd4698b04c",
              "IPY_MODEL_5378f421f5724ea1a485903ef86ab1ab"
            ],
            "layout": "IPY_MODEL_39964dacd88e4bcbb928c89b2dbcc881"
          }
        },
        "d3edbba7acc045d2bdab0246612f64e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b01716e526bc408590eaebf69fff28bc",
            "placeholder": "​",
            "style": "IPY_MODEL_e4aa98683f474409bec29ec56ba2bb2e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "4d377113fa5946bb8545abcd4698b04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7032b7bfed4f52aa1adcd92e89aa5d",
            "max": 1787,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ff343b7bb8e40d0885f047e06bbdce3",
            "value": 1787
          }
        },
        "5378f421f5724ea1a485903ef86ab1ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a61ea1a4edc46f8a79955bb1ba03bd9",
            "placeholder": "​",
            "style": "IPY_MODEL_290f3ca321104a03a308d54d7b819d95",
            "value": " 1.79k/1.79k [00:00&lt;00:00, 139kB/s]"
          }
        },
        "39964dacd88e4bcbb928c89b2dbcc881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b01716e526bc408590eaebf69fff28bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4aa98683f474409bec29ec56ba2bb2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e7032b7bfed4f52aa1adcd92e89aa5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff343b7bb8e40d0885f047e06bbdce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a61ea1a4edc46f8a79955bb1ba03bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290f3ca321104a03a308d54d7b819d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39f7749483fc478f84c773ca710496b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08ee93e4c30945baa2f43d4e8d27932f",
              "IPY_MODEL_e7c21ec05d9d42c291e1d3e6ef07df8c",
              "IPY_MODEL_48050cb3681c416a9133c2b4bdbd86c1"
            ],
            "layout": "IPY_MODEL_27255bad577f423a9b71cc7e07083164"
          }
        },
        "08ee93e4c30945baa2f43d4e8d27932f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_915c85a766944163abde5d02b5a50e2f",
            "placeholder": "​",
            "style": "IPY_MODEL_cdc743fb4301400eba1202f10e2f16da",
            "value": "config.json: 100%"
          }
        },
        "e7c21ec05d9d42c291e1d3e6ef07df8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9011a5dfcd6b4d9e87cc06447ee251eb",
            "max": 656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfb0850e58af4fc0b07684db35994047",
            "value": 656
          }
        },
        "48050cb3681c416a9133c2b4bdbd86c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ce85a1978045bea09adeae48151062",
            "placeholder": "​",
            "style": "IPY_MODEL_42d857d3872840e6a34995329f6a581a",
            "value": " 656/656 [00:00&lt;00:00, 44.1kB/s]"
          }
        },
        "27255bad577f423a9b71cc7e07083164": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915c85a766944163abde5d02b5a50e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc743fb4301400eba1202f10e2f16da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9011a5dfcd6b4d9e87cc06447ee251eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfb0850e58af4fc0b07684db35994047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2ce85a1978045bea09adeae48151062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42d857d3872840e6a34995329f6a581a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef38048833d4075af4079ab593f5440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94a14caff14748ccbfb04c7bde5da7ac",
              "IPY_MODEL_ac16e44cd43e4192b1bd3eb2e8ff0f0f",
              "IPY_MODEL_9a82b7ad532c4a128a283f8e83e0ad52"
            ],
            "layout": "IPY_MODEL_710fd42de216404290df1d7561de6baf"
          }
        },
        "94a14caff14748ccbfb04c7bde5da7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d9d070ce9c34df48b095f56c5b3d60a",
            "placeholder": "​",
            "style": "IPY_MODEL_2672158f62814f5ab436c62e0d4e64e0",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ac16e44cd43e4192b1bd3eb2e8ff0f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7383dd5feb714c11a68248d5b6ba7c57",
            "max": 2416373051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf7d6b92f6ca4e48b9ae23a40ccdaee1",
            "value": 2416373051
          }
        },
        "9a82b7ad532c4a128a283f8e83e0ad52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8ef4df0620f4503acbd4292e9086ba8",
            "placeholder": "​",
            "style": "IPY_MODEL_b97f57da6c084a43b1307727488c552f",
            "value": " 2.42G/2.42G [00:08&lt;00:00, 296MB/s]"
          }
        },
        "710fd42de216404290df1d7561de6baf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9d070ce9c34df48b095f56c5b3d60a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2672158f62814f5ab436c62e0d4e64e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7383dd5feb714c11a68248d5b6ba7c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7d6b92f6ca4e48b9ae23a40ccdaee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8ef4df0620f4503acbd4292e9086ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97f57da6c084a43b1307727488c552f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SnP EDA + Data Preparation"
      ],
      "metadata": {
        "id": "mwLOoIWGGeO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing SnP PPI data"
      ],
      "metadata": {
        "id": "DC4QEgfYQ4lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zynLVGuVUcoX",
        "outputId": "19d5d431-3745-417c-9b10-3214ed36f7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/old_dat/')"
      ],
      "metadata": {
        "id": "5U9QcJZBGhek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW16eGTAGu8W",
        "outputId": "ea902409-e988-47d2-87ce-a2eee673f9b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'all_species_PPIs_famdiversity_06_11_2023 (1).csv'   ppi-snp.csv\n",
            " all_species_PPIs_famdiversity_06_11_2023.csv\t     receptor_seqs_dict.pkl\n",
            " binder_seqs_dict.pkl\t\t\t\t     testing_dataset.csv\n",
            " complex.csv\t\t\t\t\t     testing_dataset.gsheet\n",
            " human-ppi-uniprot.csv\t\t\t\t     training_dataset.csv\n",
            " peptide_complex_test_dataset.csv\t\t     validation_dataset.csv\n",
            " peptide_complex_train_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bvFGo4UwGxR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_snp = pd.read_csv('testing_dataset.csv')\n",
        "train_snp = pd.read_csv('training_dataset.csv')\n",
        "val_snp = pd.read_csv('validation_dataset.csv')"
      ],
      "metadata": {
        "id": "Blr2K7MqGzLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_snp_data(file_path):\n",
        "    # Read the dataset\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Function to transform energy scores\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            # Replace sequences of spaces/newlines with a comma\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            # Remove a comma after an opening square bracket\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            # Remove leading commas/whitespace\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    # Apply transformations\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    # Calculate lengths for other columns\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    # Calculate matching lengths count (optional, depending on your needs)\n",
        "    snp_df['matching_lengths_count'] = (snp_df['energy_scores_lengths'] == snp_df['peptide_derived_seq_length']).sum()\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "# Applying the preprocessing pipeline to each dataset\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n"
      ],
      "metadata": {
        "id": "2MCMFq5RNk0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset + dataloaders (includes 1-hot encoding of energy scores)"
      ],
      "metadata": {
        "id": "rHWPv3RHUKNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_energy_scores(scores):\n",
        "    # Assuming 'scores' is a list of energy score values\n",
        "    return [1 if score <= -1 else 0 for score in scores]"
      ],
      "metadata": {
        "id": "cbdEeD0IWU4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "Wi6zdAfxpQ8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe, protT5_embeddings, tokenizer):\n",
        "        self.dataframe = dataframe\n",
        "        self.protT5_embeddings = protT5_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        peptide_seq = self.dataframe.iloc[idx]['peptide_derived_sequence']\n",
        "        protein_seq = self.dataframe.iloc[idx]['protein_derived_sequence']\n",
        "        energy_scores = self.dataframe.iloc[idx]['energy_scores']\n",
        "\n",
        "        # Tokenize the protein_seq\n",
        "        tokenized_protein_seq = self.tokenizer.encode(protein_seq, add_special_tokens=True, return_tensors=\"pt\").squeeze()\n",
        "\n",
        "        # Use regular expression to split the energy_scores string\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "\n",
        "        # Convert the split strings to floats\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "\n",
        "        # One-hot encode the energy scores\n",
        "        encoded_scores = one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        peptide_embedding = self.protT5_embeddings[peptide_seq]\n",
        "        protein_embedding = self.protT5_embeddings[protein_seq]\n",
        "\n",
        "        return peptide_embedding, protein_embedding, torch.tensor(encoded_scores), tokenized_protein_seq\n"
      ],
      "metadata": {
        "id": "lvXdV-LnWiiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re\n",
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfacGWrTWyVW",
        "outputId": "d8e88819-c449-45d6-a8cf-877bf1d42bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
        "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "# model = model.half() if device.type == 'cuda' else model.full()\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_protT5_embeddings(sequences):\n",
        "    embeddings_dict = {}\n",
        "    # Wrap the sequence iteration with tqdm for a progress bar\n",
        "    for sequence in tqdm(sequences, desc=\"Generating embeddings\"):\n",
        "        # Process sequence\n",
        "        processed_seq = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
        "        # Tokenize and encode\n",
        "        ids = tokenizer(processed_seq, add_special_tokens=True, return_tensors=\"pt\")\n",
        "        input_ids = ids['input_ids'].to(device)\n",
        "        attention_mask = ids['attention_mask'].to(device)\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings_dict[sequence] = embedding_repr.last_hidden_state.squeeze().mean(dim=0)\n",
        "    return embeddings_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "c058d1f4c2f040ce99f79c4d098cd04f",
            "06d6c46aa1d24995b83b5a6b1014c5e5",
            "42afd35a144145cfa265376c201b7d21",
            "d9528ec078704c0ab316305d0ce3df0c",
            "f940f97a24d642a987130af57718fefa",
            "f0616e32d6c248098a272c6d381420c0",
            "1aaa5709eb8744eba86621ce16224a67",
            "d9047c95d53142829cca25b9b8ef5855",
            "05e1fa61b5ac4b1a904c5830d2bca967",
            "2593fb85d12049a1a443dde518df23eb",
            "62c62a59206444a19b2d04311a035c02",
            "28fd24f1ccee4c218b53c1494425f475",
            "293c8524a7d64bb3aeaa6a97d0e91fbd",
            "20b561c2a68d4826ba7fddd15b419a67",
            "c4cde7c3c9924ad2b862b813bc00451f",
            "5d9fb69f0a704196bdc34f8ca10b7e66",
            "a17d7017efdf41d4bd7c6ae20451e9a7",
            "2b750a20848049a3a21d7fc9d733bf4b",
            "5af3d35e159c439e8cec2875852e0cc7",
            "b0a7a347daf3493b82d4e754c870d7f7",
            "752ad9dcfb7b4b4aaeea8bbaf191d184",
            "44d16fd5c58f4cfbaa93891a5c266ccc",
            "256a8e461e294dc88c50dc37cbf9ca16",
            "d3edbba7acc045d2bdab0246612f64e6",
            "4d377113fa5946bb8545abcd4698b04c",
            "5378f421f5724ea1a485903ef86ab1ab",
            "39964dacd88e4bcbb928c89b2dbcc881",
            "b01716e526bc408590eaebf69fff28bc",
            "e4aa98683f474409bec29ec56ba2bb2e",
            "6e7032b7bfed4f52aa1adcd92e89aa5d",
            "9ff343b7bb8e40d0885f047e06bbdce3",
            "2a61ea1a4edc46f8a79955bb1ba03bd9",
            "290f3ca321104a03a308d54d7b819d95",
            "39f7749483fc478f84c773ca710496b5",
            "08ee93e4c30945baa2f43d4e8d27932f",
            "e7c21ec05d9d42c291e1d3e6ef07df8c",
            "48050cb3681c416a9133c2b4bdbd86c1",
            "27255bad577f423a9b71cc7e07083164",
            "915c85a766944163abde5d02b5a50e2f",
            "cdc743fb4301400eba1202f10e2f16da",
            "9011a5dfcd6b4d9e87cc06447ee251eb",
            "bfb0850e58af4fc0b07684db35994047",
            "b2ce85a1978045bea09adeae48151062",
            "42d857d3872840e6a34995329f6a581a",
            "1ef38048833d4075af4079ab593f5440",
            "94a14caff14748ccbfb04c7bde5da7ac",
            "ac16e44cd43e4192b1bd3eb2e8ff0f0f",
            "9a82b7ad532c4a128a283f8e83e0ad52",
            "710fd42de216404290df1d7561de6baf",
            "9d9d070ce9c34df48b095f56c5b3d60a",
            "2672158f62814f5ab436c62e0d4e64e0",
            "7383dd5feb714c11a68248d5b6ba7c57",
            "cf7d6b92f6ca4e48b9ae23a40ccdaee1",
            "a8ef4df0620f4503acbd4292e9086ba8",
            "b97f57da6c084a43b1307727488c552f"
          ]
        },
        "id": "bftXbDHgobnZ",
        "outputId": "791d85a6-50eb-4e2c-a78d-8e93fcd6a4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c058d1f4c2f040ce99f79c4d098cd04f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/238k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28fd24f1ccee4c218b53c1494425f475"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "256a8e461e294dc88c50dc37cbf9ca16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39f7749483fc478f84c773ca710496b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ef38048833d4075af4079ab593f5440"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Generate ProtT5 embeddings\n",
        "# Combine unique sequences from peptide and protein derived sequences\n",
        "# unique_seqs = pd.concat([train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "#                          test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence'],\n",
        "#                          val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence']]).unique()\n",
        "# protT5_embeddings = generate_protT5_embeddings(unique_seqs)\n"
      ],
      "metadata": {
        "id": "3lIEygBuWj04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/')"
      ],
      "metadata": {
        "id": "WjATBhv4qP6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "f5oHt2ITqzYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'protT5_embeddings.pkl'\n",
        "with open(file_path, 'rb') as file:\n",
        "    protT5_embeddings = pickle.load(file)"
      ],
      "metadata": {
        "id": "fevkqsx2qa0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create datasets with tokenizer\n",
        "train_dataset = ProteinInteractionDataset(train_snp, protT5_embeddings, tokenizer)\n",
        "test_dataset = ProteinInteractionDataset(test_snp, protT5_embeddings, tokenizer)\n",
        "val_dataset = ProteinInteractionDataset(val_snp, protT5_embeddings, tokenizer)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wjBZHQvGqOAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save as .pkl"
      ],
      "metadata": {
        "id": "RZjMKRH2jWea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# # protT5_embeddings is a dictionary of tensors\n",
        "# with open('protT5_embeddings.pkl', 'wb') as f:\n",
        "#     pickle.dump(protT5_embeddings, f)\n"
      ],
      "metadata": {
        "id": "J7hT8gVrjTfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(protT5_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq4yEHNUkQx9",
        "outputId": "5160b81a-8764-407a-ab2f-f902d07292d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the datasets\n",
        "with open('train_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(train_dataset, f)\n",
        "\n",
        "with open('test_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(test_dataset, f)\n",
        "\n",
        "with open('val_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(val_dataset, f)"
      ],
      "metadata": {
        "id": "1QNPQKyfjYTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload Data"
      ],
      "metadata": {
        "id": "qTIHQLGqMbzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgRuef6LV4vX",
        "outputId": "0cca0d53-2de0-45b9-b3c1-96c405148275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/')"
      ],
      "metadata": {
        "id": "yYWpzJvuVjAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re\n",
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk-fPZIoV_ou",
        "outputId": "506e3305-24f8-48ec-bed5-8c773759b3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_energy_scores(scores):\n",
        "    # Assuming 'scores' is a list of energy score values\n",
        "    return [1 if score <= -1 else 0 for score in scores]"
      ],
      "metadata": {
        "id": "uAwYbJiBNtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pickle"
      ],
      "metadata": {
        "id": "xNIkI2LXN154"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe, protT5_embeddings, tokenizer):\n",
        "        self.dataframe = dataframe\n",
        "        self.protT5_embeddings = protT5_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        peptide_seq = self.dataframe.iloc[idx]['peptide_derived_sequence']\n",
        "        protein_seq = self.dataframe.iloc[idx]['protein_derived_sequence']\n",
        "        energy_scores = self.dataframe.iloc[idx]['energy_scores']\n",
        "\n",
        "        # Tokenize the protein_seq\n",
        "        tokenized_protein_seq = self.tokenizer.encode(protein_seq, add_special_tokens=True, return_tensors=\"pt\").squeeze()\n",
        "\n",
        "        # Use regular expression to split the energy_scores string\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "\n",
        "        # Convert the split strings to floats\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "\n",
        "        # One-hot encode the energy scores\n",
        "        encoded_scores = one_hot_encode_energy_scores(energy_scores)\n",
        "\n",
        "        peptide_embedding = self.protT5_embeddings[peptide_seq]\n",
        "        protein_embedding = self.protT5_embeddings[protein_seq]\n",
        "\n",
        "        return peptide_embedding, protein_embedding, torch.tensor(encoded_scores), tokenized_protein_seq\n"
      ],
      "metadata": {
        "id": "MnDIF_rxNv7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import pad\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_length_embeddings = max(max(prot1_emb.size(0), prot2_emb.size(0)) for prot1_emb, prot2_emb, _, _ in batch)\n",
        "    max_length_tokenized = max(tok_seq.size(0) for _, _, _, tok_seq in batch)\n",
        "\n",
        "    print(\"Max length for embeddings:\", max_length_embeddings)\n",
        "    print(\"Max length for tokenized sequences:\", max_length_tokenized)\n",
        "\n",
        "    prot1_embeddings_padded = []\n",
        "    prot2_embeddings_padded = []\n",
        "    one_hot_scores_padded_list = []\n",
        "    tokenized_seqs_padded = []\n",
        "\n",
        "    for prot1_emb, prot2_emb, one_hot_scores, tok_seq in batch:\n",
        "        print(\"Original sizes:\", prot1_emb.size(), prot2_emb.size(), one_hot_scores.size(), tok_seq.size())\n",
        "\n",
        "        prot1_emb_padded = pad(prot1_emb, (0, max_length_embeddings - prot1_emb.size(0)), \"constant\", 0)\n",
        "        prot2_emb_padded = pad(prot2_emb, (0, max_length_embeddings - prot2_emb.size(0)), \"constant\", 0)\n",
        "\n",
        "        if one_hot_scores.nelement() != 0:\n",
        "            one_hot_scores_padded = pad(one_hot_scores, (0, max_length_embeddings - one_hot_scores.size(0)), \"constant\", 0)\n",
        "        else:\n",
        "            one_hot_scores_padded = one_hot_scores\n",
        "\n",
        "        tok_seq_padded = pad(tok_seq, (0, max_length_tokenized - tok_seq.size(0)), \"constant\", 0)\n",
        "\n",
        "        print(\"Padded sizes:\", prot1_emb_padded.size(), prot2_emb_padded.size(), one_hot_scores_padded.size(), tok_seq_padded.size())\n",
        "\n",
        "        prot1_embeddings_padded.append(prot1_emb_padded)\n",
        "        prot2_embeddings_padded.append(prot2_emb_padded)\n",
        "        one_hot_scores_padded_list.append(one_hot_scores_padded)\n",
        "        tokenized_seqs_padded.append(tok_seq_padded)\n",
        "\n",
        "    return torch.stack(prot1_embeddings_padded), torch.stack(prot2_embeddings_padded), torch.stack(one_hot_scores_padded_list), torch.stack(tokenized_seqs_padded)\n"
      ],
      "metadata": {
        "id": "ZeEG_GoRjjZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
        "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model = model.half() if device.type == 'cuda' else model.full()\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2brgQ9ITtVxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80b958e-1de2-4a1e-845f-8cac09447878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Load the datasets\n",
        "with open('train_dataset.pkl', 'rb') as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open('test_dataset.pkl', 'rb') as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open('val_dataset.pkl', 'rb') as f:\n",
        "    val_dataset = pickle.load(f)\n",
        "\n",
        "# Assuming these are the batch sizes you want to use\n",
        "train_batch_size = 2\n",
        "test_batch_size = 2\n",
        "val_batch_size = 2\n"
      ],
      "metadata": {
        "id": "rNN3etLkMeJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAMz-Pv2fh-a",
        "outputId": "c4cdfbde-2c4a-4160-ed67-73a1d0f9d7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0517, -0.0203,  0.0323,  ..., -0.0052,  0.0547,  0.0482],\n",
              "        device='cuda:0', dtype=torch.float16),\n",
              " tensor([ 0.0432, -0.0195,  0.0399,  ...,  0.0008,  0.0552,  0.0399],\n",
              "        device='cuda:0', dtype=torch.float16),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]),\n",
              " tensor([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "WGS0LWw9OCSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motif-guided ProtFlamingo"
      ],
      "metadata": {
        "id": "lTgNrBvYVs8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions + Gated Cross Attn + Perceiver Resampler"
      ],
      "metadata": {
        "id": "QsWWUVuFs8MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# from transformers import RobertaModel  # Assuming use of Hugging Face's transformer models\n",
        "\n",
        "# Helper Functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def set_module_requires_grad_(module, requires_grad):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "def freeze_model_and_make_eval_(model):\n",
        "    model.eval()\n",
        "    set_module_requires_grad_(model, False)\n",
        "\n",
        "# LayerNorm class\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps)\n",
        "\n",
        "# Residual class\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "# SwiGLU activation function\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.silu(x[..., :x.shape[-1] // 2]) * x[..., x.shape[-1] // 2:]\n",
        "\n",
        "# Transformer Block class\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(dim, heads)\n",
        "        self.ln2 = LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            SwiGLU(),\n",
        "            nn.Linear(mlp_dim // 2, dim)\n",
        "        )\n",
        "        self.residual = Residual(self.ln1)\n",
        "        self.feedforward = Residual(self.ln2)\n",
        "        self.expand_dim = nn.Linear(dim, 2 * dim)  # Project to a higher dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() < 3: ### do the 4,1,1024 transformation\n",
        "            # Apply the expansion transformation if x has less than 3 dimensions\n",
        "            x_expanded = self.expand_dim(x)  # Now [2, 2*desired_dim]\n",
        "            x_expanded = x_expanded.view(4, 1, 1024)  # Reshape to [4, 1, 1024]\n",
        "            # x_expanded = nn.LayerNorm(x)\n",
        "            print('x transformed shape in gated cross attn:', x_expanded.shape)\n",
        "            x = self.residual(self.attn(x_expanded, x_expanded, x_expanded)[0])\n",
        "        else:\n",
        "            x = self.residual(self.attn(x, x, x)[0])\n",
        "        print(\"Shape after attention and residual:\", x.shape)  # Debug print\n",
        "        x = self.feedforward(self.mlp(x))\n",
        "        print(\"Shape after feedforward:\", x.shape)  # Debug print\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tDQcNuG8n5nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ypy2EZWLTogB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9faa53f8-7b6b-4e14-ff8a-1ba2e482610e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops-exts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDcDefLVW5-h",
        "outputId": "91504db1-d81d-4ebc-f38b-67a915081f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops-exts) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096'\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops_exts import rearrange_many, repeat_many\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def FeedForward(dim, mult = 4):\n",
        "    inner_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, inner_dim, bias = False),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(inner_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        # Adjusted to use concatenated_dim for normalization\n",
        "        self.norm_media = nn.LayerNorm(concatenated_dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "\n",
        "        # Adjusted dimensions for the larger concatenated input\n",
        "        self.to_q = nn.Linear(concatenated_dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(concatenated_dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        # Normalize x and latents\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        print(\"Original Shape of x:\", x.shape)\n",
        "        print(\"Original Shape of latents:\", latents.shape)\n",
        "\n",
        "        # Expand x to 4D if it's 3D: [B, T, D] -> [B, T, 1, D]\n",
        "        if x.dim() == 3:\n",
        "            x = x.unsqueeze(2)\n",
        "            print(\"Expanded Shape of x:\", x.shape)\n",
        "\n",
        "        # Check dimensions of latents and expand if necessary\n",
        "        if latents.dim() == 3:\n",
        "            b, m, d = latents.shape\n",
        "            latents = latents.unsqueeze(1)  # Add time dimension\n",
        "            print(\"Expanded Shape of latents:\", latents.shape)\n",
        "        elif latents.dim() != 4:\n",
        "            raise ValueError(f\"Unexpected number of dimensions in latents: {latents.dim()}\")\n",
        "\n",
        "        # Ensure x and latents have the same last dimension\n",
        "        if x.size(-1) != latents.size(-1):\n",
        "            # Use a linear layer to transform the dimension of latents\n",
        "            linear_transform = nn.Linear(latents.size(-1), x.size(-1)).to(latents.device)\n",
        "            latents = linear_transform(latents)\n",
        "            print(\"Transformed latents with linear layer:\", latents.shape)\n",
        "\n",
        "        # Concatenate x and latents along the new dimension\n",
        "        kv_input = torch.cat((x, latents), dim=2)\n",
        "        print(\"Shape of concatenated kv_input:\", kv_input.shape)\n",
        "\n",
        "\n",
        "        # Generate queries from latents, and keys and values from the concatenated input\n",
        "        q = self.to_q(latents)\n",
        "        q = rearrange(q, 'b t m (h d) -> b h t m d', h=self.heads)\n",
        "        print(\"Shape of query tensors:\", q.shape)\n",
        "\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "        k, v = rearrange_many((k, v), 'b t n (h d) -> b h t n d', h=self.heads)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        # attention\n",
        "\n",
        "        sim = einsum('... i d, ... j d  -> ... i j', q, k)\n",
        "\n",
        "        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
        "        attn = sim.softmax(dim = -1)\n",
        "\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h t n d -> b t n (h d)', h = self.heads)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, *, dim, depth, dim_head=64, heads=8, num_latents=64, num_media_embeds=4, ff_mult=4, concatenated_dim=2048):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.media_pos_emb = nn.Parameter(torch.randn(num_media_embeds, 1, concatenated_dim))\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, concatenated_dim=concatenated_dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim, mult=ff_mult)\n",
        "            ]))\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3:\n",
        "            x = rearrange(x, 'b n d -> b 1 n d')\n",
        "\n",
        "        times = x.shape[1]\n",
        "        x = x + self.media_pos_emb[:times]\n",
        "\n",
        "        latents = repeat(self.latents, 'n d -> b m n d', b=x.shape[0], m=x.shape[1])\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return self.norm(latents)\n",
        "\n",
        "\n",
        "class MaskedCrossAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim=2048, dim_head=64, heads=8, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "        self.only_attend_immediate_media = only_attend_immediate_media\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        b, t, m = media.shape[:3]\n",
        "        h = self.heads\n",
        "        print('x before norm (masked cross attn):',x.shape)\n",
        "        print('media before reaarange (masked cross attn):',media.shape)\n",
        "        x = self.norm(x)\n",
        "        q = self.to_q(x)\n",
        "        media = rearrange(media, 'b t n d -> b (t n) d')\n",
        "        k, v = self.to_kv(media).chunk(2, dim=-1)\n",
        "        print(\"Shape of q (masked cross attn):\", q.shape)\n",
        "        print(\"Shape of k (masked cross attn):\", k.shape)\n",
        "        print(\"Shape of v (masked cross attn):\", v.shape)\n",
        "\n",
        "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        q = q * self.scale\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "        if media_locations is not None:\n",
        "            # Modify attention mask based on motif presence\n",
        "            mask = media_locations.unsqueeze(1).unsqueeze(2)\n",
        "            sim = sim.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, *, dim, dim_head=64, heads=8, ff_mult=4, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.attn = MaskedCrossAttention(dim=dim, dim_head=dim_head, heads=heads, only_attend_immediate_media=only_attend_immediate_media)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "        self.expand_dim = nn.Linear(dim, 2 * dim)  # Project to a higher dimension\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        gate = self.attn_gate.tanh()\n",
        "        print('defined gate (tanh)')\n",
        "        print('gate shape:',gate.shape)\n",
        "        print('x shape initially input to gate:', x.shape)\n",
        "        # x = x.unsqueeze(1)\n",
        "        # print('x unsqueezed shape:', x.shape)\n",
        "\n",
        "        if x.dim() < 3: ### do the 4,1,1024 transformation\n",
        "            # Apply the expansion transformation if x has less than 3 dimensions\n",
        "            x_expanded = self.expand_dim(x)  # Now [2, 2*desired_dim]\n",
        "            x_expanded = x_expanded.view(4, 1, 1024)  # Reshape to [4, 1, 1024]\n",
        "            # x_expanded = nn.LayerNorm(x)\n",
        "            print('x transformed shape in gated cross attn:', x_expanded.shape)\n",
        "            x = self.attn(x_expanded, media, media_locations=media_locations) * gate + x_expanded\n",
        "        else:\n",
        "            # If x has 3 dimensions, use it as is\n",
        "            x = self.attn(x, media, media_locations=media_locations) * gate + x\n",
        "        print('self attn of x *gate +x :',x.shape)\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        print('ff*gate +x of x:', x.shape)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RwoWz-10p__C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ProtFlamingo"
      ],
      "metadata": {
        "id": "Up2gOmc-tC09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(x)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return F.silu(gate) * x\n",
        "\n",
        "class ParallelTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "        attn_inner_dim = dim_head * heads\n",
        "        ff_inner_dim = dim * ff_mult\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, 2* ff_mult * dim),\n",
        "            SwiGLU(),\n",
        "            nn.Linear(ff_mult * dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input to ParallelTransformerBlock:\", x.shape)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        print(\"After LayerNorm:\", x.shape)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # Rearrange for nn.MultiheadAttention\n",
        "        print(\"After permute for MultiheadAttention:\", x.shape)\n",
        "\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        print(\"After MultiheadAttention:\", attn_output.shape)\n",
        "\n",
        "        x = attn_output + x\n",
        "        print(\"After adding attn_output:\", x.shape)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # Rearrange back\n",
        "        print(\"After permute back:\", x.shape)\n",
        "\n",
        "        # ff_output = self.ff(x)\n",
        "        # print(\"After FeedForward:\", ff_output.shape)\n",
        "        ff_output = x\n",
        "        for layer in self.ff:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                print(\"Input to Linear Layer:\", ff_output.shape)\n",
        "                ff_output = layer(ff_output)\n",
        "                print(\"Output from Linear Layer:\", ff_output.shape)\n",
        "            else:\n",
        "                # Assuming SwiGLU or other non-linear layers don't change shape\n",
        "                ff_output = layer(ff_output)\n",
        "\n",
        "        output = ff_output + x\n",
        "        print(\"Output from ParallelTransformerBlock:\", output.shape)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "2DvByxwAW-rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "HqVTAkETnF01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "XwIGXZqUnTZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\").config.d_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i00XlIu_l5F2",
        "outputId": "065fd5ca-6036-4fef-e1b7-a32cd9fa6681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "class ProtFlamingo(nn.Module):\n",
        "    def __init__(self, num_tokens, depth, dim_head=64, heads=8, ff_mult=4, cross_attn_every=3, perceiver_num_latents=64, perceiver_depth=2, only_attend_immediate_media=True, protein_mode=False, motif_mode=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.protT5 = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.dim = 1024  # Assuming the embedding dimension\n",
        "        self.to_logits = nn.Linear(self.dim, num_tokens)\n",
        "\n",
        "        self.perceiver_resampler = PerceiverResampler(dim=self.dim, depth=perceiver_depth, dim_head=dim_head, heads=heads, num_latents=perceiver_num_latents,concatenated_dim=2048)  # new parameter for the concatenated size\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            self.layers.append(TransformerBlock(dim=self.dim, heads=heads, mlp_dim=self.dim * ff_mult))\n",
        "            if i % cross_attn_every == 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult))\n",
        "\n",
        "    def forward(self, protein_embeddings, motif_encodings, target_sequence):\n",
        "        # Print the shape of inputs\n",
        "        print(\"Protein Embeddings Shape:\", protein_embeddings.shape)\n",
        "        print(\"Motif Encodings Shape:\", motif_encodings.shape)\n",
        "        print(\"Target Sequence Shape:\", target_sequence.shape)\n",
        "\n",
        "        # Concatenate protein_embeddings and motif_encodings\n",
        "        combined_input = torch.cat((protein_embeddings, motif_encodings), dim=-1)\n",
        "        print(\"Combined Input Shape:\", combined_input.shape)\n",
        "\n",
        "        # Process the combined input through the perceiver resampler\n",
        "        processed_input = self.perceiver_resampler(combined_input)\n",
        "        print(\"Processed Input Shape:\", processed_input.shape)\n",
        "\n",
        "        # Iterate over the layers\n",
        "        for index, layer in enumerate(self.layers):\n",
        "            print(f\"Layer {index} type: {type(layer).__name__}\")\n",
        "\n",
        "            # Process the target_sequence through the layer\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                target_sequence = layer(target_sequence, processed_input)\n",
        "            else:\n",
        "                target_sequence = layer(target_sequence)\n",
        "\n",
        "            # Print the shape after each layer\n",
        "            print(f\"Post Layer {index} Shape:\", target_sequence.shape)\n",
        "\n",
        "        # Generate logits\n",
        "        logits = self.to_logits(target_sequence)\n",
        "        print(\"Logits Shape:\", logits.shape)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "lLM3qSP9q4mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "rHcPbrabtj5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs = zip(*batch)\n",
        "\n",
        "    seq1_embeddings = pad_sequence(seq1_embeddings, batch_first=True)\n",
        "    seq2_embeddings = pad_sequence(seq2_embeddings, batch_first=True)\n",
        "    one_hot_scores = pad_sequence(one_hot_scores, batch_first=True)\n",
        "    tokenized_seqs = pad_sequence(tokenized_seqs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs\n"
      ],
      "metadata": {
        "id": "tJsZbI-lUGqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import kl_div, log_softmax\n"
      ],
      "metadata": {
        "id": "VU_keTs7E9LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming 'model', 'train_dataloader', 'val_dataloader', 'test_dataloader', and 'criterion' are already defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model = model.half() if device.type == 'cuda' else model.full()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n"
      ],
      "metadata": {
        "id": "7wav5_kRU764"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader.dataset"
      ],
      "metadata": {
        "id": "mViEhzU-pfcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bb6ae3-551b-4966-f764-3b2e86ca0d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.ProteinInteractionDataset at 0x7a3ee2422f80>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ProtT5 model\n",
        "protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")"
      ],
      "metadata": {
        "id": "v7I4inNjtqVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model, optimizer, and other training components\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Example parameters\n",
        "num_tokens = protT5_tokenizer.vocab_size\n",
        "depth = 12  # Adjust based on model complexity and computational resources"
      ],
      "metadata": {
        "id": "xYk_a6w5r7c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ProtFlamingo(\n",
        "    num_tokens=num_tokens,\n",
        "    depth=depth,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    ff_mult=4,\n",
        "    cross_attn_every=2,\n",
        "    perceiver_num_latents=64,\n",
        "    perceiver_depth=2,\n",
        "    only_attend_immediate_media=True\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "upD3b5shsN3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_kl(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs in data_loader:\n",
        "        # seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs = \\\n",
        "        #     # Convert all tensors to FloatTensor\n",
        "        seq1_embeddings = seq1_embeddings.float().to(device)\n",
        "        seq2_embeddings = seq2_embeddings.float().to(device)\n",
        "        one_hot_scores = one_hot_scores.float().to(device)\n",
        "        tokenized_seqs = tokenized_seqs.float().to(device)  # Convert tokenized_seqs to float\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        model_output = model(seq1_embeddings, seq2_embeddings, one_hot_scores)\n",
        "        print('model output shape:', model_output.shape)\n",
        "        log_probs = F.log_softmax(model_output, dim=-1)\n",
        "        print('log probs shape:', log_probs.shape)\n",
        "        print('tokenized_seqs shape:', tokenized_seqs.shape)\n",
        "        print(tokenized_seqs)\n",
        "        print(log_probs)\n",
        "\n",
        "        loss = F.kl_div(log_probs, tokenized_seqs, reduction='batchmean')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Train for one epoch\n",
        "train_loss = train_epoch_kl(model, train_dataloader, optimizer, device)\n",
        "print(f\"Training Epoch: Loss = {train_loss}\")\n",
        "\n",
        "def validate_epoch_kl(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs in data_loader:\n",
        "            # seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs = \\\n",
        "            #     # Convert all tensors to FloatTensor\n",
        "            seq1_embeddings = seq1_embeddings.float().to(device)\n",
        "            seq2_embeddings = seq2_embeddings.float().to(device)\n",
        "            one_hot_scores = one_hot_scores.float().to(device)\n",
        "            tokenized_seqs = tokenized_seqs.float().to(device)  # Convert tokenized_seqs to float\n",
        "\n",
        "            model_output = model(seq1_embeddings, seq2_embeddings, one_hot_scores)\n",
        "            log_probs = F.log_softmax(model_output, dim=-1)\n",
        "\n",
        "            loss = F.kl_div(log_probs, tokenized_seqs, reduction='batchmean')\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Validate for one epoch\n",
        "val_loss = validate_epoch_kl(model, val_dataloader, device)\n",
        "print(f\"Validation Epoch: Loss = {val_loss}\")\n",
        "\n",
        "\n",
        "def test_epoch_kl(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs in data_loader:\n",
        "            # seq1_embeddings, seq2_embeddings, one_hot_scores, tokenized_seqs = \\\n",
        "            #     # Convert all tensors to FloatTensor\n",
        "            seq1_embeddings = seq1_embeddings.float().to(device)\n",
        "            seq2_embeddings = seq2_embeddings.float().to(device)\n",
        "            one_hot_scores = one_hot_scores.float().to(device)\n",
        "            tokenized_seqs = tokenized_seqs.float().to(device)  # Convert tokenized_seqs to float\n",
        "\n",
        "            model_output = model(seq1_embeddings, seq2_embeddings, one_hot_scores)\n",
        "            log_probs = F.log_softmax(model_output, dim=-1)\n",
        "\n",
        "            loss = F.kl_div(log_probs, tokenized_seqs, reduction='batchmean')\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Test for one epoch\n",
        "test_loss = test_epoch_kl(model, test_dataloader, device)\n",
        "print(f\"Test Epoch: Loss = {test_loss}\")\n"
      ],
      "metadata": {
        "id": "70J2emtxr6ZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "559534ad-3ce7-4d0a-8e5d-341bf712363a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length for embeddings: 1024\n",
            "Max length for tokenized sequences: 2\n",
            "Original sizes: torch.Size([1024]) torch.Size([1024]) torch.Size([173]) torch.Size([2])\n",
            "Padded sizes: torch.Size([1024]) torch.Size([1024]) torch.Size([1024]) torch.Size([2])\n",
            "Original sizes: torch.Size([1024]) torch.Size([1024]) torch.Size([301]) torch.Size([2])\n",
            "Padded sizes: torch.Size([1024]) torch.Size([1024]) torch.Size([1024]) torch.Size([2])\n",
            "Protein Embeddings Shape: torch.Size([2, 1024])\n",
            "Motif Encodings Shape: torch.Size([2, 1024])\n",
            "Target Sequence Shape: torch.Size([2, 1024])\n",
            "Combined Input Shape: torch.Size([2, 2048])\n",
            "Original Shape of x: torch.Size([4, 2, 2048])\n",
            "Original Shape of latents: torch.Size([4, 2, 64, 1024])\n",
            "Expanded Shape of x: torch.Size([4, 2, 1, 2048])\n",
            "Transformed latents with linear layer: torch.Size([4, 2, 64, 2048])\n",
            "Shape of concatenated kv_input: torch.Size([4, 2, 65, 2048])\n",
            "Shape of query tensors: torch.Size([4, 8, 2, 64, 64])\n",
            "Original Shape of x: torch.Size([4, 2, 2048])\n",
            "Original Shape of latents: torch.Size([4, 2, 64, 1024])\n",
            "Expanded Shape of x: torch.Size([4, 2, 1, 2048])\n",
            "Transformed latents with linear layer: torch.Size([4, 2, 64, 2048])\n",
            "Shape of concatenated kv_input: torch.Size([4, 2, 65, 2048])\n",
            "Shape of query tensors: torch.Size([4, 8, 2, 64, 64])\n",
            "Processed Input Shape: torch.Size([4, 2, 64, 1024])\n",
            "Layer 0 type: TransformerBlock\n",
            "x transformed shape in gated cross attn: torch.Size([4, 1, 1024])\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 0 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 1 type: GatedCrossAttentionBlock\n",
            "defined gate (tanh)\n",
            "gate shape: torch.Size([1])\n",
            "x shape initially input to gate: torch.Size([4, 1, 1024])\n",
            "x before norm (masked cross attn): torch.Size([4, 1, 1024])\n",
            "media before reaarange (masked cross attn): torch.Size([4, 2, 64, 1024])\n",
            "Shape of q (masked cross attn): torch.Size([4, 1, 512])\n",
            "Shape of k (masked cross attn): torch.Size([4, 128, 512])\n",
            "Shape of v (masked cross attn): torch.Size([4, 128, 512])\n",
            "self attn of x *gate +x : torch.Size([4, 1, 1024])\n",
            "ff*gate +x of x: torch.Size([4, 1, 1024])\n",
            "Post Layer 1 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 2 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 2 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 3 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 3 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 4 type: GatedCrossAttentionBlock\n",
            "defined gate (tanh)\n",
            "gate shape: torch.Size([1])\n",
            "x shape initially input to gate: torch.Size([4, 1, 1024])\n",
            "x before norm (masked cross attn): torch.Size([4, 1, 1024])\n",
            "media before reaarange (masked cross attn): torch.Size([4, 2, 64, 1024])\n",
            "Shape of q (masked cross attn): torch.Size([4, 1, 512])\n",
            "Shape of k (masked cross attn): torch.Size([4, 128, 512])\n",
            "Shape of v (masked cross attn): torch.Size([4, 128, 512])\n",
            "self attn of x *gate +x : torch.Size([4, 1, 1024])\n",
            "ff*gate +x of x: torch.Size([4, 1, 1024])\n",
            "Post Layer 4 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 5 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 5 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 6 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 6 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 7 type: GatedCrossAttentionBlock\n",
            "defined gate (tanh)\n",
            "gate shape: torch.Size([1])\n",
            "x shape initially input to gate: torch.Size([4, 1, 1024])\n",
            "x before norm (masked cross attn): torch.Size([4, 1, 1024])\n",
            "media before reaarange (masked cross attn): torch.Size([4, 2, 64, 1024])\n",
            "Shape of q (masked cross attn): torch.Size([4, 1, 512])\n",
            "Shape of k (masked cross attn): torch.Size([4, 128, 512])\n",
            "Shape of v (masked cross attn): torch.Size([4, 128, 512])\n",
            "self attn of x *gate +x : torch.Size([4, 1, 1024])\n",
            "ff*gate +x of x: torch.Size([4, 1, 1024])\n",
            "Post Layer 7 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 8 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 8 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 9 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 9 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 10 type: GatedCrossAttentionBlock\n",
            "defined gate (tanh)\n",
            "gate shape: torch.Size([1])\n",
            "x shape initially input to gate: torch.Size([4, 1, 1024])\n",
            "x before norm (masked cross attn): torch.Size([4, 1, 1024])\n",
            "media before reaarange (masked cross attn): torch.Size([4, 2, 64, 1024])\n",
            "Shape of q (masked cross attn): torch.Size([4, 1, 512])\n",
            "Shape of k (masked cross attn): torch.Size([4, 128, 512])\n",
            "Shape of v (masked cross attn): torch.Size([4, 128, 512])\n",
            "self attn of x *gate +x : torch.Size([4, 1, 1024])\n",
            "ff*gate +x of x: torch.Size([4, 1, 1024])\n",
            "Post Layer 10 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 11 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 11 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 12 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 12 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 13 type: GatedCrossAttentionBlock\n",
            "defined gate (tanh)\n",
            "gate shape: torch.Size([1])\n",
            "x shape initially input to gate: torch.Size([4, 1, 1024])\n",
            "x before norm (masked cross attn): torch.Size([4, 1, 1024])\n",
            "media before reaarange (masked cross attn): torch.Size([4, 2, 64, 1024])\n",
            "Shape of q (masked cross attn): torch.Size([4, 1, 512])\n",
            "Shape of k (masked cross attn): torch.Size([4, 128, 512])\n",
            "Shape of v (masked cross attn): torch.Size([4, 128, 512])\n",
            "self attn of x *gate +x : torch.Size([4, 1, 1024])\n",
            "ff*gate +x of x: torch.Size([4, 1, 1024])\n",
            "Post Layer 13 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 14 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 14 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 15 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 15 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 16 type: GatedCrossAttentionBlock\n",
            "defined gate (tanh)\n",
            "gate shape: torch.Size([1])\n",
            "x shape initially input to gate: torch.Size([4, 1, 1024])\n",
            "x before norm (masked cross attn): torch.Size([4, 1, 1024])\n",
            "media before reaarange (masked cross attn): torch.Size([4, 2, 64, 1024])\n",
            "Shape of q (masked cross attn): torch.Size([4, 1, 512])\n",
            "Shape of k (masked cross attn): torch.Size([4, 128, 512])\n",
            "Shape of v (masked cross attn): torch.Size([4, 128, 512])\n",
            "self attn of x *gate +x : torch.Size([4, 1, 1024])\n",
            "ff*gate +x of x: torch.Size([4, 1, 1024])\n",
            "Post Layer 16 Shape: torch.Size([4, 1, 1024])\n",
            "Layer 17 type: TransformerBlock\n",
            "Shape after attention and residual: torch.Size([4, 1, 1024])\n",
            "Shape after feedforward: torch.Size([4, 1, 1024])\n",
            "Post Layer 17 Shape: torch.Size([4, 1, 1024])\n",
            "Logits Shape: torch.Size([4, 1, 28])\n",
            "model output shape: torch.Size([4, 1, 28])\n",
            "log probs shape: torch.Size([4, 1, 28])\n",
            "tokenized_seqs shape: torch.Size([2, 2])\n",
            "tensor([[2., 1.],\n",
            "        [2., 1.]], device='cuda:0')\n",
            "tensor([[[-4.3220, -5.3770, -5.2589, -4.3895, -4.4273, -3.1028, -1.2441,\n",
            "          -4.6104, -3.8534, -4.1344, -2.8862, -3.4676, -3.6808, -4.9375,\n",
            "          -4.4179, -3.4985, -2.1165, -5.1027, -3.4162, -5.1421, -3.5789,\n",
            "          -3.4645, -2.3307, -4.3653, -3.9063, -3.5124, -3.8789, -5.1761]],\n",
            "\n",
            "        [[-4.3220, -5.3770, -5.2589, -4.3895, -4.4273, -3.1028, -1.2441,\n",
            "          -4.6104, -3.8534, -4.1344, -2.8862, -3.4676, -3.6808, -4.9375,\n",
            "          -4.4179, -3.4985, -2.1165, -5.1027, -3.4162, -5.1421, -3.5789,\n",
            "          -3.4645, -2.3307, -4.3653, -3.9063, -3.5124, -3.8789, -5.1761]],\n",
            "\n",
            "        [[-4.3220, -5.3770, -5.2589, -4.3895, -4.4273, -3.1028, -1.2441,\n",
            "          -4.6104, -3.8534, -4.1344, -2.8862, -3.4676, -3.6808, -4.9375,\n",
            "          -4.4179, -3.4985, -2.1165, -5.1027, -3.4162, -5.1421, -3.5789,\n",
            "          -3.4645, -2.3307, -4.3653, -3.9063, -3.5124, -3.8789, -5.1761]],\n",
            "\n",
            "        [[-4.3220, -5.3770, -5.2589, -4.3895, -4.4273, -3.1028, -1.2441,\n",
            "          -4.6104, -3.8534, -4.1344, -2.8862, -3.4676, -3.6808, -4.9375,\n",
            "          -4.4179, -3.4985, -2.1165, -5.1027, -3.4162, -5.1421, -3.5789,\n",
            "          -3.4645, -2.3307, -4.3653, -3.9063, -3.5124, -3.8789, -5.1761]]],\n",
            "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-00704cc8baac>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_kl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Epoch: Loss = {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-00704cc8baac>\u001b[0m in \u001b[0;36mtrain_epoch_kl\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batchmean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mkl_div\u001b[0;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[1;32m   2953\u001b[0m             \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2955\u001b[0;31m     \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batchmean\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (28) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "protT5_tokenizer"
      ],
      "metadata": {
        "id": "so-4BEHlgsV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
