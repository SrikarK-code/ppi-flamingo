{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7rhuRCHcBuW"
      },
      "source": [
        "### **Make a new dataset**\n",
        "- target seq\n",
        "- binder seq\n",
        "- motif seq\n",
        "- cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raFP6jfJKxXF",
        "outputId": "130f264e-4ed0-457d-81f8-68949eeb23de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re\n",
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w137bz4JVPB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/per-residue-dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibs4RGaLKoZI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_snp_data(file_path):\n",
        "    # Read the dataset\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Function to transform energy scores\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            # Replace sequences of spaces/newlines with a comma\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            # Remove a comma after an opening square bracket\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            # Remove leading commas/whitespace\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    # Apply transformations\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    # Calculate lengths for other columns\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    # Calculate matching lengths count (optional, depending on your needs)\n",
        "    snp_df['matching_lengths_count'] = (snp_df['energy_scores_lengths'] == snp_df['peptide_derived_seq_length']).sum()\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "# Applying the preprocessing pipeline to each dataset\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0llJJwgaK5F2"
      },
      "outputs": [],
      "source": [
        "unique_seqs = pd.concat([train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "                         test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence'],\n",
        "                         val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence']]).unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(seq) for seq in unique_seqs)\n",
        "print(max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsucu112U37m",
        "outputId": "872753ff-5465-48cd-91ee-ca6ffc832ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJAJtKEzcV1j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        peptide_seq = self.dataframe.iloc[idx]['peptide_derived_sequence']\n",
        "        protein_seq = self.dataframe.iloc[idx]['protein_derived_sequence']\n",
        "        energy_scores = self.dataframe.iloc[idx]['energy_scores']\n",
        "\n",
        "        max_length = 984\n",
        "\n",
        "        # Process the energy_scores\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = one_hot_encode_energy_scores(energy_scores)\n",
        "        energy_scores_padded = pad(torch.tensor(energy_scores), (0, max_length - len(energy_scores)), \"constant\", 0)\n",
        "\n",
        "        return energy_scores_padded, peptide_seq, protein_seq\n",
        "\n",
        "def one_hot_encode_energy_scores(scores):\n",
        "        # Assuming 'scores' is a list of energy score values\n",
        "        return [1 if score <= -1 else 0 for score in scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQHCeqEiK5yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cb9b1b-162b-449c-889e-041004300fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "# Create datasets with tokenizer\n",
        "train_dataset = ProteinInteractionDataset(train_snp)\n",
        "test_dataset = ProteinInteractionDataset(test_snp)\n",
        "val_dataset = ProteinInteractionDataset(val_snp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHVnA7vpLTAw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_batch_size = 1\n",
        "test_batch_size = 1\n",
        "val_batch_size = 1\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8pMULBaC_qS"
      },
      "source": [
        "### **Background Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General"
      ],
      "metadata": {
        "id": "wagP-3zdrZyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ACmWOYMDFLA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# from transformers import RobertaModel  # Assuming use of Hugging Face's transformer models\n",
        "\n",
        "# Helper Functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def set_module_requires_grad_(module, requires_grad):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "def freeze_model_and_make_eval_(model):\n",
        "    model.eval()\n",
        "    set_module_requires_grad_(model, False)\n",
        "\n",
        "# LayerNorm class\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps)\n",
        "\n",
        "# Residual class\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "# SwiGLU activation function\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.silu(x[..., :x.shape[-1] // 2]) * x[..., x.shape[-1] // 2:]\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(x)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return F.silu(gate) * x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Perciever+Cross Attn"
      ],
      "metadata": {
        "id": "x-rLhOM9rbsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install einops-exts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv4u1pLEQhdW",
        "outputId": "d5648e05-a539-4eec-d2d1-e691e43c2582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops-exts) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ichz74-ADmOv"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096'\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops_exts import rearrange_many, repeat_many\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def FeedForward(dim, mult = 4):\n",
        "    inner_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, inner_dim, bias = False),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(inner_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # print('x shape perciever attn:', x.shape)\n",
        "        # print('latents shape perceiver attn', latents.shape)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        # print('q shape:',q.shape)\n",
        "\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        q = q * self.scale\n",
        "\n",
        "\n",
        "        kv_input = torch.cat((x, latents), dim=1)\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "\n",
        "        # print('k shape:',k.shape)\n",
        "        # print('v shape:',v.shape)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        # print('rearrangement in perceiver cross attn complete...')\n",
        "        # print('q shape:',q.shape)\n",
        "        # print('k shape:',k.shape)\n",
        "        # print('v shape:',v.shape)\n",
        "\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, *, dim, depth, dim_head=64, heads=8, num_latents=64, concatenated_dim=1536):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, concatenated_dim=concatenated_dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=x.shape[0])\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class MaskedCrossAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim=1536, dim_head=64, heads=8, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "        self.only_attend_immediate_media = only_attend_immediate_media\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        b, t, _ = x.shape\n",
        "        _, m, _ = media.shape\n",
        "        h = self.heads\n",
        "\n",
        "        x = self.norm(x)\n",
        "        q = self.to_q(x)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        # No need to reshape media as it's already 3D\n",
        "        k, v = self.to_kv(media).chunk(2, dim=-1)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        q = q * self.scale\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "\n",
        "        if media_locations is not None:\n",
        "            mask = media_locations.unsqueeze(1).unsqueeze(2)\n",
        "            mask = rearrange(mask, 'b n -> b 1 n 1')\n",
        "            sim = sim.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=self.heads)\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, *, dim, dim_head=64, heads=8, ff_mult=4, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.attn = MaskedCrossAttention(dim=dim, concatenated_dim=1536, dim_head=dim_head, heads=heads, only_attend_immediate_media=only_attend_immediate_media)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(x, media, media_locations=media_locations) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJBI5IltgLPW"
      },
      "source": [
        "### **ProtFlamingo**\n",
        "- input = tokenized target,binder & motif encoding\n",
        "- protT5 embed tokenized AA seqs (text), motif emb (image)\n",
        "- goal: complete binder seq (text completion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5EncoderModel\n",
        "\n",
        "class ProtFlamingoLearnedEmbedding(nn.Module):\n",
        "    def __init__(self, num_tokens, depth, dim_head=64, heads=8, ff_mult=4, cross_attn_every=3, perceiver_num_latents=64, perceiver_depth=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the device for the model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.motif_embedding_projection = nn.Embedding(2, 768)  # Binary one-hot encoding to 768 dimensions\n",
        "\n",
        "        # Load ProtT5 model and tokenizer\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "\n",
        "        # Access the decoder blocks from ProtT5 model\n",
        "        self.decoder_blocks = self.protT5_model.decoder.block\n",
        "\n",
        "        # Intersperse GatedCrossAttentionBlocks within the T5 decoder blocks\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            self.layers.append(block)\n",
        "            if i % cross_attn_every == 0 and i != 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.protT5_model.config.d_model, dim_head=dim_head, heads=heads))\n",
        "\n",
        "        self.perceiver_resampler = PerceiverResampler(dim=self.protT5_model.config.d_model, depth=perceiver_depth, dim_head=dim_head, heads=heads, num_latents=perceiver_num_latents)\n",
        "        self.expand_seq_len = nn.Linear(dim_head, 984)\n",
        "\n",
        "    def forward(self, target_seqs, binder_seqs, motif_encodings):\n",
        "        MAX_LEN = 984\n",
        "        all_logits_pred = []\n",
        "        all_binder_tokenized_padded = []\n",
        "        for i in range(len(target_seqs)):\n",
        "            target_seq = target_seqs[i]\n",
        "            binder_seq = binder_seqs[i]\n",
        "            motif_one_hot = motif_encodings[i]\n",
        "            motif_one_hot = motif_one_hot.unsqueeze(0)\n",
        "\n",
        "            print('Target Seq:',target_seq)\n",
        "            print('Motif Encoding Shape:', motif_one_hot.shape)\n",
        "\n",
        "            target_embeddings, target_tokenized, target_attn_mask = self.generate_protT5_embeddings_tokens(target_seq)\n",
        "            print('target_emb shape before padding:', target_embeddings.shape)\n",
        "            binder_embeddings, binder_tokenized, binder_attn_mask = self.generate_protT5_embeddings_tokens(binder_seq)\n",
        "\n",
        "            # Padding embeddings and tokenized sequences\n",
        "            target_embeddings_padded = pad(target_embeddings, (0, 0, 0, MAX_LEN - target_embeddings.size(1)), \"constant\", 0)\n",
        "            binder_embeddings_padded = pad(binder_embeddings, (0, 0, 0, MAX_LEN - binder_embeddings.size(1)), \"constant\", 0)\n",
        "\n",
        "            target_tokenized_padded = pad(torch.tensor(target_tokenized, dtype=torch.long), (0, MAX_LEN - len(target_tokenized)), \"constant\", 0)\n",
        "            binder_tokenized_padded = pad(torch.tensor(binder_tokenized, dtype=torch.long), (0, MAX_LEN - len(binder_tokenized)), \"constant\", 0)\n",
        "\n",
        "            # target_tokenized_padded = target_tokenized_padded.unsqueeze(0)\n",
        "            # binder_tokenized_padded = binder_tokenized_padded.unsqueeze(0)\n",
        "\n",
        "            # Padding attention masks\n",
        "            target_attn_mask_padded = pad(target_attn_mask, (0, MAX_LEN - target_attn_mask.size(1)), \"constant\", 0)\n",
        "            binder_attn_mask_padded = pad(binder_attn_mask, (0, MAX_LEN - binder_attn_mask.size(1)), \"constant\", 0)\n",
        "\n",
        "            motif_embeddings = self.motif_embedding_projection(motif_one_hot.long()) # this should be 1,984,768\n",
        "            # motif 1 != 2 mismatch is causing assertion error below!\n",
        "            #print(\"Motif Embeddings:\",motif_embeddings)\n",
        "\n",
        "            processed_motif_embeddings = self.perceiver_resampler(motif_embeddings)\n",
        "\n",
        "            # Pass through layers (T5Blocks and GatedCrossAttentionBlocks)\n",
        "            # Process through layers\n",
        "            for layer in self.layers:\n",
        "                if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                    print('starting gated cross attn block...')\n",
        "                    print('input motif emb shape:', processed_motif_embeddings.shape)\n",
        "                    print('input target emb shape:', target_embeddings_padded.shape)\n",
        "                    target_embeddings_padded = layer(target_embeddings_padded, processed_motif_embeddings)\n",
        "                    print('passed thru gated cross attn block...')\n",
        "                    print(\"  Output from Gated Cross Attn Block is:\", target_embeddings_padded.shape)\n",
        "                    if isinstance(target_embeddings_padded, tuple):\n",
        "                      print(\"  Output from Gated Cross Attn Block is a tuple. Taking first element.\")\n",
        "                      target_embeddings_padded = target_embeddings_padded[0]\n",
        "                      print('output shape:', target_embeddings_padded.shape)\n",
        "                else:\n",
        "                    print('starting t5 block...')\n",
        "                    print('input target emb shape:', target_embeddings_padded.shape)\n",
        "                    print(\"input attn mask shape:\",target_attn_mask_padded.shape)\n",
        "                    target_embeddings_padded = layer(target_embeddings_padded, attention_mask=target_attn_mask_padded)\n",
        "\n",
        "                    print('passed thru t5block...')\n",
        "                    if isinstance(target_embeddings_padded, tuple):\n",
        "                      print(\"  Output from T5 Decoder Block is a tuple. Taking first element.\")\n",
        "                      target_embeddings_padded = target_embeddings_padded[0]\n",
        "                      print('output shape:', target_embeddings_padded.shape)\n",
        "\n",
        "            logits = self.protT5_model.lm_head(target_embeddings_padded)\n",
        "            print('Output Logits:',logits)\n",
        "            print(\"Output Logits Shape:\", logits.shape)\n",
        "\n",
        "            all_logits_pred.append(logits)\n",
        "            all_binder_tokenized_padded.append(binder_tokenized_padded)\n",
        "\n",
        "        # Combine results from all batch elements\n",
        "        batch_binder_tokenized_padded = torch.stack(all_binder_tokenized_padded, dim=0)\n",
        "        batch_predicted_logits = torch.stack(all_logits_pred, dim=0)\n",
        "        print('Batch pred logits:', batch_predicted_logits.shape)\n",
        "        print('GT Binder Toks:', batch_binder_tokenized_padded.shape)\n",
        "\n",
        "        return batch_binder_tokenized_padded, batch_predicted_logits\n",
        "\n",
        "    def generate_protT5_embeddings_tokens(self, sequence):\n",
        "        processed_seq = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
        "        ids = self.protT5_tokenizer(processed_seq, add_special_tokens=True, return_tensors=\"pt\", padding='longest')\n",
        "        input_ids = ids['input_ids'].to(device)\n",
        "        seq_tok = input_ids.squeeze().tolist()\n",
        "        attention_mask = ids['attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding_repr = self.protT5_encoder_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        seq_emb = embedding_repr.last_hidden_state\n",
        "        # print('sequence embedding shape:',seq_emb.shape)\n",
        "\n",
        "        return seq_emb,seq_tok,attention_mask"
      ],
      "metadata": {
        "id": "n-F7sc-4q82h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv7mkV67D5wB"
      },
      "source": [
        "### **Initialize Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQJHfZpvD4JZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Example parameters\n",
        "num_tokens = 128 # protT5 vocab size\n",
        "depth = 3  # Adjust based on model complexity and computational resources\n",
        "\n",
        "model = ProtFlamingoLearnedEmbedding(\n",
        "    num_tokens=num_tokens,\n",
        "    depth=depth,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    ff_mult=4,\n",
        "    cross_attn_every=2,\n",
        "    perceiver_num_latents=64,\n",
        "    perceiver_depth=2\n",
        ")\n",
        "\n",
        "# Assuming 'model', 'train_dataloader', 'val_dataloader', 'test_dataloader', and 'criterion' are already defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "import torch\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == torch.nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPLbkTa8EIDQ"
      },
      "source": [
        "### **Train Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iLan9TaEG2u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_epoch_ce(model, data_loader, optimizer, device, clip_value=1.0):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for one_hot_motifs, target_seqs, binder_seqs in data_loader:\n",
        "        one_hot_motifs = one_hot_motifs.float().to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        print(\"Target sequences:\", target_seqs)\n",
        "        print(\"One hot motifs:\", one_hot_motifs)\n",
        "\n",
        "        # Forward pass\n",
        "        targets, logits = model(target_seqs, binder_seqs, one_hot_motifs)\n",
        "        logits = logits.float()  # Convert logits to float\n",
        "        targets = targets.long().to(device)  # Ensure targets are long and on the same device as logits\n",
        "\n",
        "        print('ce loss calc begins...')\n",
        "        print('input logits shape:', logits.view(-1, logits.size(-1)).shape)  # e.g., 984,128\n",
        "        print(logits)\n",
        "        print('input target shape:', targets.view(-1).shape)  # e.g., 984\n",
        "        print(targets)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients: gradients are modified in place\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "    average_loss = total_loss / total_batches if total_batches > 0 else 0\n",
        "    print(f\"Training Loss: {average_loss:.4f}\")\n",
        "    return average_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0w2IwsWNEsR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca36020e-5e6f-4c21-a7bc-400c4810a3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Target Seq: MIHHVELTQSVLQYIRDSSVRDNDILRDLREETSKLPLRTMQIPPEQGQLLSLLVRLIGARKTLEVGVFTGYSTLCTALALPADGRVIACDLSEEWVSIARRYWQRAGVADRIEVRLGDAHHSLEALVGSEHRGTFDLAFIDADKESYDFYYEHALRLVRPGGLIILDNTLWSGKVADPSVVGDPETDSLRRINAKLLTDERVDLSMLPIADGLTLARKRKL\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 223, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "Output Logits: tensor([[[ 3.2932e+01, -2.8007e+01, -3.7335e+01,  ...,  5.8384e+01,\n",
            "           5.7180e+00, -2.8141e+01],\n",
            "         [ 3.9431e+01, -1.6638e+01,  9.2281e+00,  ...,  4.1028e+01,\n",
            "           2.3474e+01, -2.5713e+00],\n",
            "         [ 3.9908e+01, -1.0108e+01, -1.2832e+01,  ...,  6.6779e+00,\n",
            "          -4.8797e-02, -2.1943e+00],\n",
            "         ...,\n",
            "         [-1.5410e+00, -6.8897e+00,  3.2387e+01,  ...,  4.0869e+01,\n",
            "          -1.4632e+01, -3.6809e+01],\n",
            "         [ 2.9649e+00, -2.0487e+01,  1.7804e+01,  ...,  3.1215e+01,\n",
            "           2.6950e+01, -2.3334e+01],\n",
            "         [-1.2756e+01, -1.3297e+01,  9.5969e+00,  ...,  7.1056e+01,\n",
            "           3.5941e+01, -3.0709e+01]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Output Logits Shape: torch.Size([1, 984, 128])\n",
            "Batch pred logits: torch.Size([1, 1, 984, 128])\n",
            "GT Binder Toks: torch.Size([1, 984])\n",
            "Target Seq: DTRPRFLEQVKHECHFFNGTERVRFLDRYFYHQEEYVRFDSDVGEYRAVTELGRPDAEYWNSQKDLLEQKRAAVDTYCRHNYGVGESFTVQRRVYPEVTVYPAKTQPLQHHNLLVCSVNGFYPGSIEVRWFRNGQEEKTGVVSTGLIQNGDWTFQTLVMLETVPRSGEVYTCQVEHPSLTSPLTVEWRA\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 190, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "Output Logits: tensor([[[ 15.4212,  -0.5955, -13.5993,  ...,  13.7532,   2.2220,  -2.0272],\n",
            "         [ 50.3903,   4.0105,  10.3454,  ...,  35.9522,  11.5767, -22.0082],\n",
            "         [ 32.0840, -25.0426,  15.8181,  ...,  35.8178,  34.6873, -28.3268],\n",
            "         ...,\n",
            "         [  0.5994, -13.8985,  30.8233,  ...,  55.9243,  25.9850,   0.4325],\n",
            "         [ 19.2911,  -0.3229,  13.2293,  ...,  18.7307,  10.9910,  -8.4394],\n",
            "         [-34.8090, -13.9837,   2.2285,  ...,  36.3333,  10.2680,   6.3143]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "Output Logits Shape: torch.Size([1, 984, 128])\n",
            "Batch pred logits: torch.Size([1, 1, 984, 128])\n",
            "GT Binder Toks: torch.Size([1, 984])\n",
            "Target Seq: DIVNRKVEHVEIAAFENVDGLSSSTFLNDVILVHQGFPGISFSEINTKTKFFRKEISVPVMVTGMTGGRNELGRINKIIAEVAEKFGIPMGVGSQRVAIEKAEARESFAIVRKVAPTIPIIANLGMPQLVKGYGLKEFQDAIQMIEADAIAVHLNPAQEVFQPEGEPEYQIYALEKLRDISKELSVPIIVKESGNGISMETAKLLYSYGIKNFDTSGQGGTNWIAIEMIRDIRRGNWKAESAKNFLDWGVPTAASIMEVRYSVPDSFLVGSGGIRSGLDAAKAIALGADIAGMALPVLKSAIEGKESLEQFFRKIIFELKAAMMLTGSKDVDALKKTSIVILGKLKEWAEYRGINLSIYEKVRK\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 365, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "Output Logits: tensor([[[  5.9448, -12.7139,  -1.8392,  ...,  17.7172,  19.8762,  -5.6071],\n",
            "         [ -1.7489,  -2.7313,   3.2164,  ...,  17.3515,   6.4842,  19.5493],\n",
            "         [ 22.7109, -35.1444,  28.3399,  ...,  14.3293,   7.3231,  -0.3985],\n",
            "         ...,\n",
            "         [ 11.7575, -25.5908,  -5.7919,  ...,  31.3162,  25.3070,  -9.4984],\n",
            "         [ 35.7049, -10.9015,  22.4561,  ...,  38.5358,   7.4863,  -9.3037],\n",
            "         [ 24.3208, -14.2097,  16.6000,  ...,  33.5885, -13.2629,  -8.3272]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "Output Logits Shape: torch.Size([1, 984, 128])\n",
            "Batch pred logits: torch.Size([1, 1, 984, 128])\n",
            "GT Binder Toks: torch.Size([1, 984])\n",
            "Target Seq: IAEPTSHDPDSGGHFGGPSGWGGRYVPEALMAVIEEVTAAYQKERVSQDFLDDLDRLQANYAGRPSPLYEATRLSQHAGSARIFLKREDLNHTGSHINNVLGQALLARRMGKTRVIAETGAGQHGVATATACALLGLDCVIYMGGIDTARQALNVARMRLLGAEVVAVQTGSKTLKDAINEAFRDWVANADNTYYCFGTAAGPHPFPTMVRDFQRIIGMEARVQIQGQAGRLPDAVVACVGGGSNAIGIFHAFLDDPGVRLVGFEAAGDGVETGRHAATFTAGSPGAFHGSFSYLLQDEDGQTIESHSISAGLDYPGVGPEHAWLKEAGRVDYRPITDSEAMDAFGLLCRMEGIIPAIESAHAVAGALKLGVELGRGAVIVVNLSGRGDKDVETAAKWFGLLGN\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 405, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "Output Logits: tensor([[[  1.3188, -15.7061, -15.5354,  ...,  19.9463,  13.2600,  -3.7547],\n",
            "         [ 18.4829,  -4.4530,   7.4587,  ...,  28.9506,  34.7720,  -5.6981],\n",
            "         [ -7.5770, -23.0663,  11.6674,  ...,  51.5868,   0.7129,   9.0281],\n",
            "         ...,\n",
            "         [ 16.2711, -19.3382,  27.4889,  ...,  25.4335,  16.5947,  12.0414],\n",
            "         [ -1.0560, -36.3519,  19.9747,  ...,  12.6655,  27.6596,   0.1785],\n",
            "         [ 18.0338, -17.8729,   1.4475,  ...,  39.8593,  34.1891, -14.6374]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "Output Logits Shape: torch.Size([1, 984, 128])\n",
            "Batch pred logits: torch.Size([1, 1, 984, 128])\n",
            "GT Binder Toks: torch.Size([1, 984])\n",
            "Target Seq: HAFVSTLTRGDLSSIRWVCSRHAQPTCPGAQLCTVYYASLNFRDIMLATGKLSPDAIPGKWTSQDSLLGMEFSGRDASGKRVMGLVPAKGLATSVLLSPDFLWDVPSNWTLEEAASVPVVYSTAYYALVVRGRVRPGETLLIHSGSGGVGQAAIAIALSLGCRVFTTVGSAEKRAYLQARFPQLDSTSFANSRSFEQHVLWHTGGKGVDLVLNSLAEEKLQASVRCLATHGRFLEIGKFDLGMAIFLKNVTFHGVLLDAFFNESSADWREVWALVQAGIRDGVVRPLKCTVFHGAQVEDAFRYMAQGKHIGKVVVQVLAEEPE\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 324, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "Output Logits: tensor([[[ 1.5683e+03, -7.4008e+01, -3.4895e+00,  ..., -1.3750e+02,\n",
            "          -1.2632e+01,  8.3265e+01],\n",
            "         [ 1.5652e+03, -5.7342e+01, -1.8382e+01,  ..., -1.5221e+02,\n",
            "           6.8259e+00,  6.3740e+01],\n",
            "         [ 1.5470e+03, -6.4241e+01, -3.5123e+01,  ..., -1.5712e+02,\n",
            "          -7.9175e+00,  6.2326e+01],\n",
            "         ...,\n",
            "         [ 1.5773e+03, -5.3275e+01, -2.6172e+01,  ..., -1.3251e+02,\n",
            "          -8.6293e-01,  6.7060e+01],\n",
            "         [ 1.5903e+03, -8.3299e+01,  3.0570e+00,  ..., -1.4259e+02,\n",
            "          -4.4811e+00,  5.3147e+01],\n",
            "         [ 1.5843e+03, -8.3368e+01, -1.2376e+01,  ..., -1.4155e+02,\n",
            "          -8.1616e+00,  7.7756e+01]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Output Logits Shape: torch.Size([1, 984, 128])\n",
            "Batch pred logits: torch.Size([1, 1, 984, 128])\n",
            "GT Binder Toks: torch.Size([1, 984])\n",
            "Target Seq: TIFEKKPDFTLFLQTLSWEIDDQVGIEVRNELLREVGRGGTRIPPPCQTVDKLQIELNALLALIGWGTVTLELLSEDQSLRIVHENLPQVGSAGEPSGTWLAPVLEGLYGRWVTSQAGAFGDYVVTRDVDAEDLNAVPRQTIIYRVRSSAT\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 152, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 32.81 MiB is free. Process 128549 has 39.52 GiB memory in use. Of the allocated memory 38.33 GiB is allocated by PyTorch, and 713.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-55e650b8ee03>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_ce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-11ccd74dd415>\u001b[0m in \u001b[0;36mtrain_epoch_ce\u001b[0;34m(model, data_loader, optimizer, device, clip_value, accumulation_steps)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinder_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_motifs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert logits to float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure targets are long and on the same device as logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-6a56a1657511>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, target_seqs, binder_seqs, motif_encodings)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input target emb shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_embeddings_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input attn mask shape:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_attn_mask_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mtarget_embeddings_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_embeddings_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_attn_mask_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'passed thru t5block...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 32.81 MiB is free. Process 128549 has 39.52 GiB memory in use. Of the allocated memory 38.33 GiB is allocated by PyTorch, and 713.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Lists to store losses\n",
        "train_losses = []\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    train_loss = train_epoch_ce(model, train_dataloader, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B8pMULBaC_qS",
        "iv7mkV67D5wB"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}