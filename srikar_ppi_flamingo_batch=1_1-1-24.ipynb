{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7rhuRCHcBuW"
      },
      "source": [
        "### **Make a new dataset**\n",
        "- target seq\n",
        "- binder seq\n",
        "- motif seq\n",
        "- cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raFP6jfJKxXF",
        "outputId": "bd043ecc-8605-40c3-e8bf-c7005e1c2b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "import re\n",
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4w137bz4JVPB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Programmable Biology Group/Srikar/Code/proteins/flamingo-ppi-gen/data_dump/per-residue-dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ibs4RGaLKoZI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_snp_data(file_path):\n",
        "    # Read the dataset\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Function to transform energy scores\n",
        "    def transform_energy_scores(energy_scores):\n",
        "        transformed_scores = []\n",
        "        for score in energy_scores:\n",
        "            # Replace sequences of spaces/newlines with a comma\n",
        "            score = re.sub(r'[\\s\\n]+', ',', score)\n",
        "            # Remove a comma after an opening square bracket\n",
        "            score = re.sub(r'\\[\\s*,', '[', score)\n",
        "            # Remove leading commas/whitespace\n",
        "            score = re.sub(r'^[\\s,]+', '', score)\n",
        "            transformed_scores.append(score)\n",
        "        return transformed_scores\n",
        "\n",
        "    # Apply transformations\n",
        "    snp_df['energy_scores'] = transform_energy_scores(snp_df['energy_scores'])\n",
        "    snp_df['energy_scores_lengths'] = snp_df['energy_scores'].apply(\n",
        "        lambda x: x.count(',') + 1 - (1 if x.startswith(',') else 0)\n",
        "    )\n",
        "\n",
        "    # Calculate lengths for other columns\n",
        "    snp_df['peptide_source_RCSB_lengths'] = snp_df['peptide_source_RCSB'].apply(len)\n",
        "    snp_df['protein_RCSB_lengths'] = snp_df['protein_RCSB'].apply(len)\n",
        "    snp_df['protein_derived_seq_length'] = snp_df['protein_derived_sequence'].apply(len)\n",
        "    snp_df['peptide_derived_seq_length'] = snp_df['peptide_derived_sequence'].apply(len)\n",
        "\n",
        "    # Calculate matching lengths count (optional, depending on your needs)\n",
        "    snp_df['matching_lengths_count'] = (snp_df['energy_scores_lengths'] == snp_df['peptide_derived_seq_length']).sum()\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "# Applying the preprocessing pipeline to each dataset\n",
        "test_snp = preprocess_snp_data('testing_dataset.csv')\n",
        "train_snp = preprocess_snp_data('training_dataset.csv')\n",
        "val_snp = preprocess_snp_data('validation_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0llJJwgaK5F2"
      },
      "outputs": [],
      "source": [
        "unique_seqs = pd.concat([train_snp['peptide_derived_sequence'], train_snp['protein_derived_sequence'],\n",
        "                         test_snp['peptide_derived_sequence'], test_snp['protein_derived_sequence'],\n",
        "                         val_snp['peptide_derived_sequence'], val_snp['protein_derived_sequence']]).unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(seq) for seq in unique_seqs)\n",
        "print(max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsucu112U37m",
        "outputId": "aa151201-1f86-41fc-eb77-1b3f19ab0c72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EJAJtKEzcV1j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        peptide_seq = self.dataframe.iloc[idx]['peptide_derived_sequence']\n",
        "        protein_seq = self.dataframe.iloc[idx]['protein_derived_sequence']\n",
        "        energy_scores = self.dataframe.iloc[idx]['energy_scores']\n",
        "\n",
        "        max_length = 984\n",
        "\n",
        "        # Process the energy_scores\n",
        "        energy_scores = re.findall(r'-?\\d+\\.?\\d*(?:e[-+]?\\d+)?', energy_scores)\n",
        "        energy_scores = [float(score) for score in energy_scores]\n",
        "        energy_scores = one_hot_encode_energy_scores(energy_scores)\n",
        "        energy_scores_padded = pad(torch.tensor(energy_scores), (0, max_length - len(energy_scores)), \"constant\", 0)\n",
        "\n",
        "        return energy_scores_padded, peptide_seq, protein_seq\n",
        "\n",
        "def one_hot_encode_energy_scores(scores):\n",
        "        # Assuming 'scores' is a list of energy score values\n",
        "        return [1 if score <= -1 else 0 for score in scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uQHCeqEiK5yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cd784f-7e75-4df2-e2dc-dcbb63c57c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "# Create datasets with tokenizer\n",
        "train_dataset = ProteinInteractionDataset(train_snp)\n",
        "test_dataset = ProteinInteractionDataset(test_snp)\n",
        "val_dataset = ProteinInteractionDataset(val_snp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BHVnA7vpLTAw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_batch_size = 1\n",
        "test_batch_size = 1\n",
        "val_batch_size = 1\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8pMULBaC_qS"
      },
      "source": [
        "### **Background Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General"
      ],
      "metadata": {
        "id": "wagP-3zdrZyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1ACmWOYMDFLA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# from transformers import RobertaModel  # Assuming use of Hugging Face's transformer models\n",
        "\n",
        "# Helper Functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def set_module_requires_grad_(module, requires_grad):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "def freeze_model_and_make_eval_(model):\n",
        "    model.eval()\n",
        "    set_module_requires_grad_(model, False)\n",
        "\n",
        "# LayerNorm class\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps)\n",
        "\n",
        "# Residual class\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "# SwiGLU activation function\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.silu(x[..., :x.shape[-1] // 2]) * x[..., x.shape[-1] // 2:]\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(x)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return F.silu(gate) * x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Perciever+Cross Attn"
      ],
      "metadata": {
        "id": "x-rLhOM9rbsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install einops-exts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv4u1pLEQhdW",
        "outputId": "54d947d6-ef54-4c46-a600-6f56d98fdcbf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops-exts) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ichz74-ADmOv"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:4096'\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops_exts import rearrange_many, repeat_many\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def FeedForward(dim, mult = 4):\n",
        "    inner_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim),\n",
        "        nn.Linear(dim, inner_dim, bias = False),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(inner_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # print('x shape perciever attn:', x.shape)\n",
        "        # print('latents shape perceiver attn', latents.shape)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        # print('q shape:',q.shape)\n",
        "\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        q = q * self.scale\n",
        "\n",
        "\n",
        "        kv_input = torch.cat((x, latents), dim=1)\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "\n",
        "        # print('k shape:',k.shape)\n",
        "        # print('v shape:',v.shape)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        # print('rearrangement in perceiver cross attn complete...')\n",
        "        # print('q shape:',q.shape)\n",
        "        # print('k shape:',k.shape)\n",
        "        # print('v shape:',v.shape)\n",
        "\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, *, dim, depth, dim_head=64, heads=8, num_latents=64, concatenated_dim=2048):\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, concatenated_dim=concatenated_dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=x.shape[0])\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class MaskedCrossAttention(nn.Module):\n",
        "    def __init__(self, *, dim, concatenated_dim=2048, dim_head=64, heads=8, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "        self.only_attend_immediate_media = only_attend_immediate_media\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        b, t, _ = x.shape\n",
        "        _, m, _ = media.shape\n",
        "        h = self.heads\n",
        "\n",
        "        x = self.norm(x)\n",
        "        q = self.to_q(x)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        # No need to reshape media as it's already 3D\n",
        "        k, v = self.to_kv(media).chunk(2, dim=-1)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
        "\n",
        "        q = q * self.scale\n",
        "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
        "\n",
        "        if media_locations is not None:\n",
        "            mask = media_locations.unsqueeze(1).unsqueeze(2)\n",
        "            mask = rearrange(mask, 'b n -> b 1 n 1')\n",
        "            sim = sim.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=self.heads)\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, *, dim, dim_head=64, heads=8, ff_mult=4, only_attend_immediate_media=True):\n",
        "        super().__init__()\n",
        "        self.attn = MaskedCrossAttention(dim=dim, concatenated_dim=2048, dim_head=dim_head, heads=heads, only_attend_immediate_media=only_attend_immediate_media)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media, media_locations=None):\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(x, media, media_locations=media_locations) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJBI5IltgLPW"
      },
      "source": [
        "### **ProtFlamingo**\n",
        "- input = tokenized target,binder & motif encoding\n",
        "- protT5 embed tokenized AA seqs (text), motif emb (image)\n",
        "- goal: complete binder seq (text completion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5EncoderModel\n",
        "\n",
        "class ProtFlamingoLearnedEmbedding(nn.Module):\n",
        "    def __init__(self, num_tokens, depth, dim_head=64, heads=8, ff_mult=4, cross_attn_every=3, perceiver_num_latents=64, perceiver_depth=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the device for the model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.motif_embedding_projection = nn.Embedding(2, 1024)  # Binary one-hot encoding to 1024 dimensions\n",
        "\n",
        "        # Load ProtT5 model and tokenizer\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_encoder_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "\n",
        "        # Access the decoder blocks from ProtT5 model\n",
        "        self.decoder_blocks = self.protT5_model.decoder.block\n",
        "\n",
        "        # Intersperse GatedCrossAttentionBlocks within the T5 decoder blocks\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            self.layers.append(block)\n",
        "            if i % cross_attn_every == 0 and i != 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.protT5_model.config.d_model, dim_head=dim_head, heads=heads))\n",
        "\n",
        "        self.perceiver_resampler = PerceiverResampler(dim=self.protT5_model.config.d_model, depth=perceiver_depth, dim_head=dim_head, heads=heads, num_latents=perceiver_num_latents)\n",
        "        self.expand_seq_len = nn.Linear(dim_head, 984)\n",
        "\n",
        "    def forward(self, target_seqs, binder_seqs, motif_encodings):\n",
        "        MAX_LEN = 984\n",
        "        all_logits_pred = []\n",
        "        all_binder_tokenized_padded = []\n",
        "        for i in range(len(target_seqs)):\n",
        "            target_seq = target_seqs[i]\n",
        "            binder_seq = binder_seqs[i]\n",
        "            motif_one_hot = motif_encodings[i]\n",
        "            motif_one_hot = motif_one_hot.unsqueeze(0)\n",
        "\n",
        "            print('Target Seq:',target_seq)\n",
        "            print('Motif Encoding Shape:', motif_one_hot.shape)\n",
        "\n",
        "            target_embeddings, target_tokenized, target_attn_mask = self.generate_protT5_embeddings_tokens(target_seq)\n",
        "            print('target_emb shape before padding:', target_embeddings.shape)\n",
        "            binder_embeddings, binder_tokenized, binder_attn_mask = self.generate_protT5_embeddings_tokens(binder_seq)\n",
        "\n",
        "            # Padding embeddings and tokenized sequences\n",
        "            target_embeddings_padded = pad(target_embeddings, (0, 0, 0, MAX_LEN - target_embeddings.size(1)), \"constant\", 0)\n",
        "            binder_embeddings_padded = pad(binder_embeddings, (0, 0, 0, MAX_LEN - binder_embeddings.size(1)), \"constant\", 0)\n",
        "\n",
        "            target_tokenized_padded = pad(torch.tensor(target_tokenized, dtype=torch.long), (0, MAX_LEN - len(target_tokenized)), \"constant\", 0)\n",
        "            binder_tokenized_padded = pad(torch.tensor(binder_tokenized, dtype=torch.long), (0, MAX_LEN - len(binder_tokenized)), \"constant\", 0)\n",
        "\n",
        "            # target_tokenized_padded = target_tokenized_padded.unsqueeze(0)\n",
        "            # binder_tokenized_padded = binder_tokenized_padded.unsqueeze(0)\n",
        "\n",
        "            # Padding attention masks\n",
        "            target_attn_mask_padded = pad(target_attn_mask, (0, MAX_LEN - target_attn_mask.size(1)), \"constant\", 0)\n",
        "            binder_attn_mask_padded = pad(binder_attn_mask, (0, MAX_LEN - binder_attn_mask.size(1)), \"constant\", 0)\n",
        "\n",
        "            motif_embeddings = self.motif_embedding_projection(motif_one_hot.long()) # this should be 1,984,1024\n",
        "            # motif 1 != 2 mismatch is causing assertion error below!\n",
        "            print(\"Motif Embeddings:\",motif_embeddings)\n",
        "\n",
        "            processed_motif_embeddings = self.perceiver_resampler(motif_embeddings)\n",
        "\n",
        "            # Pass through layers (T5Blocks and GatedCrossAttentionBlocks)\n",
        "            # Process through layers\n",
        "            for layer in self.layers:\n",
        "                if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                    print('starting gated cross attn block...')\n",
        "                    print('input motif emb shape:', processed_motif_embeddings.shape)\n",
        "                    print('input target emb shape:', target_embeddings_padded.shape)\n",
        "                    target_embeddings_padded = layer(target_embeddings_padded, processed_motif_embeddings)\n",
        "                    print('passed thru gated cross attn block...')\n",
        "                    print(\"  Output from Gated Cross Attn Block is:\", target_embeddings_padded.shape)\n",
        "                    if isinstance(target_embeddings_padded, tuple):\n",
        "                      print(\"  Output from Gated Cross Attn Block is a tuple. Taking first element.\")\n",
        "                      target_embeddings_padded = target_embeddings_padded[0]\n",
        "                      print('output shape:', target_embeddings_padded.shape)\n",
        "                else:\n",
        "                    print('starting t5 block...')\n",
        "                    print('input target emb shape:', target_embeddings_padded.shape)\n",
        "                    print(\"input attn mask shape:\",target_attn_mask_padded.shape)\n",
        "                    target_embeddings_padded = layer(target_embeddings_padded, attention_mask=target_attn_mask_padded)\n",
        "\n",
        "                    print('passed thru t5block...')\n",
        "                    if isinstance(target_embeddings_padded, tuple):\n",
        "                      print(\"  Output from T5 Decoder Block is a tuple. Taking first element.\")\n",
        "                      target_embeddings_padded = target_embeddings_padded[0]\n",
        "                      print('output shape:', target_embeddings_padded.shape)\n",
        "\n",
        "            logits = self.protT5_model.lm_head(target_embeddings_padded)\n",
        "            print('Output Logits:',logits)\n",
        "            print(\"Output Logits Shape:\", logits.shape)\n",
        "\n",
        "            all_logits_pred.append(logits)\n",
        "            all_binder_tokenized_padded.append(binder_tokenized_padded)\n",
        "\n",
        "        # Combine results from all batch elements\n",
        "        batch_binder_tokenized_padded = torch.stack(all_binder_tokenized_padded, dim=0)\n",
        "        batch_predicted_logits = torch.stack(all_logits_pred, dim=0)\n",
        "        print('Batch pred logits:', batch_predicted_logits.shape)\n",
        "        print('GT Binder Toks:', batch_binder_tokenized_padded.shape)\n",
        "\n",
        "        return batch_binder_tokenized_padded, batch_predicted_logits\n",
        "\n",
        "    def generate_protT5_embeddings_tokens(self, sequence):\n",
        "        processed_seq = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
        "        ids = self.protT5_tokenizer(processed_seq, add_special_tokens=True, return_tensors=\"pt\", padding='longest')\n",
        "        input_ids = ids['input_ids'].to(device)\n",
        "        seq_tok = input_ids.squeeze().tolist()\n",
        "        attention_mask = ids['attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding_repr = self.protT5_encoder_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        seq_emb = embedding_repr.last_hidden_state\n",
        "        # print('sequence embedding shape:',seq_emb.shape)\n",
        "\n",
        "        return seq_emb,seq_tok,attention_mask"
      ],
      "metadata": {
        "id": "n-F7sc-4q82h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv7mkV67D5wB"
      },
      "source": [
        "### **Initialize Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WQJHfZpvD4JZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Example parameters\n",
        "num_tokens = 128 # protT5 vocab size\n",
        "depth = 3  # Adjust based on model complexity and computational resources\n",
        "\n",
        "model = ProtFlamingoLearnedEmbedding(\n",
        "    num_tokens=num_tokens,\n",
        "    depth=depth,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    ff_mult=4,\n",
        "    cross_attn_every=2,\n",
        "    perceiver_num_latents=64,\n",
        "    perceiver_depth=2\n",
        ")\n",
        "\n",
        "# Assuming 'model', 'train_dataloader', 'val_dataloader', 'test_dataloader', and 'criterion' are already defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "import torch\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == torch.nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPLbkTa8EIDQ"
      },
      "source": [
        "### **Train Loop (One batch)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8iLan9TaEG2u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_epoch_ce(model, data_loader, optimizer, device, clip_value=1.0):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # Process only the first batch from the data loader\n",
        "    one_hot_motifs, target_seqs, binder_seqs = next(iter(data_loader))\n",
        "\n",
        "    one_hot_motifs = one_hot_motifs.float().to(device)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    print(\"Target sequences:\", target_seqs)\n",
        "    print(\"One hot motifs:\", one_hot_motifs)\n",
        "\n",
        "    # Forward pass\n",
        "    targets,logits = model(target_seqs, binder_seqs, one_hot_motifs)\n",
        "    logits = logits.float()  # Convert logits to float\n",
        "    targets = targets.long().to(device)  # Ensure targets are long and on the same device as logits\n",
        "\n",
        "    print('ce loss calc begins...')\n",
        "    print('input logits shape:',logits.view(-1, logits.size(-1)).shape) #984,128\n",
        "    print('input target shape:',targets.view(-1).shape) #984\n",
        "    # loss = nn.CrossEntropyLoss()(logits,targets)\n",
        "    loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "    # making sure the loss is run on 984,128 vs 984\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss\n",
        "    print(f\"Training Loss: {average_loss:.4f}\")\n",
        "    return average_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q0w2IwsWNEsR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a43b5346-600c-4466-a15d-5879336c5112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n",
            "Target sequences: ('AEKLTLEAITGSAPLSGPTLTKPQIAPDGSRVTFLRGKDRDRNRLDLWEYDIASGQTRLLVDSSVVLPGEEVLSDEEKARRERQRIAALSGIVDYQWSPDGKALLFPLGGELYFYDLTKSGRDAVRKLTNGGGFATDPKISPKGGFVSFIRDRNLWAIDLASGKEVQLTRDGSDTIGNGVAEFVADEEMDRHTGYWWAPDDAAIAFARIDETPVPVQKRYEVYPDRTEVVEQRYPAAGDHNVRVQLGVIAPKTGARPRWIDLGKDPDIYLARVDWRDPQRLTFQRQSRDQKKIELIETTLTNGTQRTLVTETSTTWVPLHNDLRFLKDGRFLWSSERSGFEHLYVASEDGSTLTALTQGEWVVDSLLAIDEAAGLAYVSGTRDGATEAHVYAVPLSGGEPRRLTQAPGMHAATFARNASVFVDSWSSDTTLPQIELFKADGTKLATLLVNDVSDATHPYAKYRAAHQPTAYGTLTAADGTTPLHYSLIKPAGFDPKKQYPVVVFVYGGPAAQTVTRAWPGRSDSFFNQYLAQQGYVVFTLDNRGTPRRGAAFGGALYGKQGTVEVDDQLRGIEWLKSQAFVDPARIGVYGWSNGGYMTLMLLAKHDEAYACGVAGAPVTDWALYDTHYTERYMDLPKANEAGYREASVFTHVDGIGAGKLLLIHGMADDNVLFTNSTKLMSELQKRGTPFELMTYPGAKHGLRGSDLLHRYRLTEDFFARCLKP',)\n",
            "One hot motifs: tensor([[1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
            "Target Seq: AEKLTLEAITGSAPLSGPTLTKPQIAPDGSRVTFLRGKDRDRNRLDLWEYDIASGQTRLLVDSSVVLPGEEVLSDEEKARRERQRIAALSGIVDYQWSPDGKALLFPLGGELYFYDLTKSGRDAVRKLTNGGGFATDPKISPKGGFVSFIRDRNLWAIDLASGKEVQLTRDGSDTIGNGVAEFVADEEMDRHTGYWWAPDDAAIAFARIDETPVPVQKRYEVYPDRTEVVEQRYPAAGDHNVRVQLGVIAPKTGARPRWIDLGKDPDIYLARVDWRDPQRLTFQRQSRDQKKIELIETTLTNGTQRTLVTETSTTWVPLHNDLRFLKDGRFLWSSERSGFEHLYVASEDGSTLTALTQGEWVVDSLLAIDEAAGLAYVSGTRDGATEAHVYAVPLSGGEPRRLTQAPGMHAATFARNASVFVDSWSSDTTLPQIELFKADGTKLATLLVNDVSDATHPYAKYRAAHQPTAYGTLTAADGTTPLHYSLIKPAGFDPKKQYPVVVFVYGGPAAQTVTRAWPGRSDSFFNQYLAQQGYVVFTLDNRGTPRRGAAFGGALYGKQGTVEVDDQLRGIEWLKSQAFVDPARIGVYGWSNGGYMTLMLLAKHDEAYACGVAGAPVTDWALYDTHYTERYMDLPKANEAGYREASVFTHVDGIGAGKLLLIHGMADDNVLFTNSTKLMSELQKRGTPFELMTYPGAKHGLRGSDLLHRYRLTEDFFARCLKP\n",
            "Motif Encoding Shape: torch.Size([1, 984])\n",
            "target_emb shape before padding: torch.Size([1, 725, 1024])\n",
            "Motif Embeddings: tensor([[[ 0.0626, -0.7327,  0.0457,  ...,  1.8096,  0.1822,  1.4162],\n",
            "         [ 0.0626, -0.7327,  0.0457,  ...,  1.8096,  0.1822,  1.4162],\n",
            "         [ 0.8646, -0.2865, -1.0243,  ...,  0.0050,  0.0711, -0.7060],\n",
            "         ...,\n",
            "         [ 0.8646, -0.2865, -1.0243,  ...,  0.0050,  0.0711, -0.7060],\n",
            "         [ 0.8646, -0.2865, -1.0243,  ...,  0.0050,  0.0711, -0.7060],\n",
            "         [ 0.8646, -0.2865, -1.0243,  ...,  0.0050,  0.0711, -0.7060]]],\n",
            "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "starting gated cross attn block...\n",
            "input motif emb shape: torch.Size([1, 64, 1024])\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "passed thru gated cross attn block...\n",
            "  Output from Gated Cross Attn Block is: torch.Size([1, 984, 1024])\n",
            "starting t5 block...\n",
            "input target emb shape: torch.Size([1, 984, 1024])\n",
            "input attn mask shape: torch.Size([1, 984])\n",
            "passed thru t5block...\n",
            "  Output from T5 Decoder Block is a tuple. Taking first element.\n",
            "output shape: torch.Size([1, 984, 1024])\n",
            "Output Logits: tensor([[[-17.1685, -26.5748,   1.1166,  ...,  11.3021,  19.3721,  41.1240],\n",
            "         [-53.3442,   9.4503,   9.2894,  ..., -11.8361,  18.2782, -16.2942],\n",
            "         [-43.1267,   2.8288, -25.1972,  ..., -42.8628,  57.0001,  17.4785],\n",
            "         ...,\n",
            "         [-12.3335,   1.7927,  -5.6993,  ...,  -4.8874,  32.1676,  -4.6975],\n",
            "         [-35.9565,   6.8288,  -1.3272,  ..., -20.0095,  27.6292,   4.4033],\n",
            "         [-32.9146, -11.3074,   6.2953,  ...,  -2.0911,  55.6594,  21.2711]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
            "Output Logits Shape: torch.Size([1, 984, 128])\n",
            "Batch pred logits: torch.Size([1, 1, 984, 128])\n",
            "GT Binder Toks: torch.Size([1, 984])\n",
            "ce loss calc begins...\n",
            "input logits shape: torch.Size([984, 128])\n",
            "input target shape: torch.Size([984])\n",
            "Training Loss: 71.2626\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Lists to store losses\n",
        "train_losses = []\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    train_loss = train_epoch_ce(model, train_dataloader, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B8pMULBaC_qS",
        "iv7mkV67D5wB"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}